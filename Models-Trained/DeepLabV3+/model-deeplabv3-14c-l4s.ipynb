{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-19T18:13:20.210420Z",
     "iopub.status.busy": "2025-02-19T18:13:20.210205Z",
     "iopub.status.idle": "2025-02-19T18:13:34.184354Z",
     "shell.execute_reply": "2025-02-19T18:13:34.183260Z",
     "shell.execute_reply.started": "2025-02-19T18:13:20.210399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\n",
      "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
      "Collecting pretrainedmodels\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\n",
      "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.14.1)\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.9.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.12.0)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: model-signing in /usr/local/lib/python3.10/dist-packages (from kagglehub) (0.2.0)\n",
      "Collecting munch (from pretrainedmodels)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.9)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.11)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from model-signing->kagglehub) (44.0.0)\n",
      "Requirement already satisfied: in-toto-attestation in /usr/local/lib/python3.10/dist-packages (from model-signing->kagglehub) (0.9.3)\n",
      "Requirement already satisfied: sigstore in /usr/local/lib/python3.10/dist-packages (from model-signing->kagglehub) (3.6.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->model-signing->kagglehub) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: id>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (1.5.0)\n",
      "Requirement already satisfied: importlib_resources~=5.7 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (5.13.0)\n",
      "Requirement already satisfied: pyasn1~=0.6 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (0.6.1)\n",
      "Requirement already satisfied: pyjwt>=2.1 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (2.10.1)\n",
      "Requirement already satisfied: pyOpenSSL>=23.0.0 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (25.0.0)\n",
      "Requirement already satisfied: rich~=13.0 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (13.9.4)\n",
      "Requirement already satisfied: rfc8785~=0.1.2 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (0.1.4)\n",
      "Requirement already satisfied: rfc3161-client~=0.1.2 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (0.1.2)\n",
      "Requirement already satisfied: sigstore-protobuf-specs==0.3.2 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (0.3.2)\n",
      "Requirement already satisfied: sigstore-rekor-types==0.0.18 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (0.0.18)\n",
      "Requirement already satisfied: tuf~=5.0 in /usr/local/lib/python3.10/dist-packages (from sigstore->model-signing->kagglehub) (5.1.0)\n",
      "Requirement already satisfied: betterproto==2.0.0b6 in /usr/local/lib/python3.10/dist-packages (from sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (2.0.0b6)\n",
      "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (0.4.8rc2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->model-signing->kagglehub) (2.22)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich~=13.0->sigstore->model-signing->kagglehub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich~=13.0->sigstore->model-signing->kagglehub) (2.19.1)\n",
      "Requirement already satisfied: securesystemslib~=1.0 in /usr/local/lib/python3.10/dist-packages (from tuf~=5.0->sigstore->model-signing->kagglehub) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich~=13.0->sigstore->model-signing->kagglehub) (0.1.2)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.7.0)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.1.0)\n",
      "Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=5a70376044176ca3870f6fbf2f74106d5fe054b48bf515cf1c0aca95267432c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=06ccd3313ab9437b0024b33809507578b6e2f81f8616321b40b8a572205c2354\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "Successfully built pretrainedmodels efficientnet-pytorch\n",
      "Installing collected packages: banal, sqlalchemy, munch, torchviz, efficientnet-pytorch, dataset, pretrainedmodels, segmentation_models_pytorch\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.36\n",
      "    Uninstalling SQLAlchemy-2.0.36:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.36\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed banal-1.0.6 dataset-1.6.2 efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0 sqlalchemy-1.4.54 torchviz-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch wandb dataset pytorch_lightning torchsummary torchviz kaggle kagglehub pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:13:34.185751Z",
     "iopub.status.busy": "2025-02-19T18:13:34.185442Z",
     "iopub.status.idle": "2025-02-19T18:13:51.033354Z",
     "shell.execute_reply": "2025-02-19T18:13:51.032269Z",
     "shell.execute_reply.started": "2025-02-19T18:13:34.185725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hks/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import h5py\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer  # Import Trainer here\n",
    "import torchmetrics\n",
    "import segmentation_models_pytorch as smp\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:13:51.034306Z",
     "iopub.status.busy": "2025-02-19T18:13:51.034098Z",
     "iopub.status.idle": "2025-02-19T18:13:51.046355Z",
     "shell.execute_reply": "2025-02-19T18:13:51.045367Z",
     "shell.execute_reply.started": "2025-02-19T18:13:51.034289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set CUDA settings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Load the configuration file\n",
    "config = \"\"\"\n",
    "model_config:\n",
    "  model_type: \"deeplabv3+\"\n",
    "  in_channels: 14\n",
    "  num_classes: 1\n",
    "  encoder_weights: \"imagenet\"\n",
    "  wce_weight: 0.5\n",
    "\n",
    "dataset_config:\n",
    "  num_classes: 1\n",
    "  num_channels: 14\n",
    "  channels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 ,12, 13, 14]\n",
    "  normalize: False\n",
    "\n",
    "train_config:\n",
    "  dataset_path: \"/kaggle/input/landslide4sense/TrainData\"\n",
    "  checkpoint_path: \"checkpoints\"\n",
    "  seed: 42\n",
    "  train_val_split: 0.8\n",
    "  batch_size: 16\n",
    "  num_epochs: 100\n",
    "  lr: 0.001\n",
    "  device: \"cuda:0\"\n",
    "  save_config: True\n",
    "  experiment_name: \"deeplabv3+\"\n",
    "\n",
    "logging_config:\n",
    "  wandb_project: \"Land4Sense\"\n",
    "  wandb_entity: \"Silvamillion\"\n",
    "\"\"\"\n",
    "\n",
    "config = yaml.safe_load(config)\n",
    "# Set the random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Utils\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    set_seed(42 + worker_id)\n",
    "\n",
    "# Dataset\n",
    "class DatasetLandslide(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.train_paths, self.mask_paths = self.read_data(path)\n",
    "\n",
    "    def read_data(self, path):\n",
    "        TRAIN_PATH = f\"{path}/img/*.h5\"\n",
    "        TRAIN_MASK = f'{path}/mask/*.h5'\n",
    "        all_train = sorted(glob.glob(TRAIN_PATH))\n",
    "        all_mask = sorted(glob.glob(TRAIN_MASK))\n",
    "        return all_train, all_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        channels = config[\"dataset_config\"][\"channels\"]\n",
    "        TRAIN_XX = np.zeros((128, 128, len(channels)))\n",
    "\n",
    "        with h5py.File(self.train_paths[idx]) as hdf:\n",
    "            data = np.array(hdf.get('img'))\n",
    "            data[np.isnan(data)] = 0.000001\n",
    "\n",
    "            for i, channel in enumerate(channels):\n",
    "                TRAIN_XX[:, :, i] = data[:, :, channel-1]  # Change here to use 0-based indexing\n",
    "                if self.train_paths[idx].find(\"image\") != -1:\n",
    "                    TRAIN_XX[:, :, i] = data[:, :, channel-1]\n",
    "\n",
    "            img = TRAIN_XX.transpose((2, 0, 1))  # Transponemos para tener (C, H, W)\n",
    "            if config[\"dataset_config\"][\"normalize\"]:\n",
    "                img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "\n",
    "        mask = np.array([])\n",
    "        if self.mask_paths != []:\n",
    "            with h5py.File(self.mask_paths[idx]) as hdf:\n",
    "                mask = np.array(hdf.get('mask'))\n",
    "                mask = mask[np.newaxis, :]  # Añadir dimensión de canal\n",
    "\n",
    "        return torch.from_numpy(img).float(), torch.from_numpy(mask).float()  # Convertimos a tensores de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:14:07.878294Z",
     "iopub.status.busy": "2025-02-19T18:14:07.878001Z",
     "iopub.status.idle": "2025-02-19T18:14:07.896253Z",
     "shell.execute_reply": "2025-02-19T18:14:07.895325Z",
     "shell.execute_reply.started": "2025-02-19T18:14:07.878274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class smp_model(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, model_type, num_classes, encoder_weights):\n",
    "        super(smp_model, self).__init__()\n",
    "        if model_type == \"deeplabv3+\":\n",
    "            self.model = smp.DeepLabV3Plus(\n",
    "                encoder_name=\"resnet50\",  # Change this to a valid encoder\n",
    "                encoder_weights=encoder_weights,\n",
    "                in_channels=in_channels,\n",
    "                classes=num_classes\n",
    "            )\n",
    "        elif model_type == \"unet\":\n",
    "            self.model = smp.Unet(\n",
    "                encoder_name=\"resnet50\",\n",
    "                encoder_weights=encoder_weights,\n",
    "                in_channels=in_channels,\n",
    "                classes=num_classes,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Model type {model_type} not supported!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LandslideModel(pl.LightningModule):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(LandslideModel, self).__init__()\n",
    "\n",
    "        model_type = config['model_config']['model_type']\n",
    "        in_channels = config['model_config']['in_channels']\n",
    "        num_classes = config['model_config']['num_classes']\n",
    "        self.alpha = alpha  # Assign the alpha value to the class variable\n",
    "        self.lr = config['train_config']['lr']\n",
    "\n",
    "        if model_type == 'unet':\n",
    "            self.model = UNet(in_channels=in_channels, out_channels=num_classes)\n",
    "        else:\n",
    "            encoder_weights = config['model_config']['encoder_weights']\n",
    "            self.model = smp_model(in_channels=in_channels, \n",
    "                                   out_channels=num_classes, \n",
    "                                   model_type=model_type, \n",
    "                                   num_classes=num_classes, \n",
    "                                   encoder_weights=encoder_weights)\n",
    "\n",
    "        self.weights = torch.tensor([5], dtype=torch.float32).to(self.device)\n",
    "        self.wce = nn.BCELoss(weight=self.weights)\n",
    "\n",
    "        self.train_f1 = torchmetrics.F1Score(task='binary')\n",
    "        self.val_f1 = torchmetrics.F1Score(task='binary')\n",
    "\n",
    "        self.train_precision = torchmetrics.Precision(task='binary')\n",
    "        self.val_precision = torchmetrics.Precision(task='binary')\n",
    "\n",
    "        self.train_recall = torchmetrics.Recall(task='binary')\n",
    "        self.val_recall = torchmetrics.Recall(task='binary')\n",
    "\n",
    "        self.train_iou = torchmetrics.JaccardIndex(task='binary')\n",
    "        self.val_iou = torchmetrics.JaccardIndex(task='binary')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = torch.sigmoid(self(x))\n",
    "\n",
    "        wce_loss = self.wce(y_hat, y)\n",
    "        dice = dice_loss(y_hat, y)\n",
    "\n",
    "        combined_loss = (1 - self.alpha) * wce_loss + self.alpha * dice\n",
    "\n",
    "        precision = self.train_precision(y_hat, y)\n",
    "        recall = self.train_recall(y_hat, y)\n",
    "        iou = self.train_iou(y_hat, y)\n",
    "        loss_f1 = self.train_f1(y_hat, y)\n",
    "\n",
    "        self.log('train_precision', precision)\n",
    "        self.log('train_recall', recall)\n",
    "        self.log('train_wce', wce_loss)\n",
    "        self.log('train_dice', dice)\n",
    "        self.log('train_iou', iou)\n",
    "        self.log('train_f1', loss_f1)\n",
    "        self.log('train_loss', combined_loss)\n",
    "        return {'loss': combined_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = torch.sigmoid(self(x))\n",
    "\n",
    "        wce_loss = self.wce(y_hat, y)\n",
    "        dice = dice_loss(y_hat, y)\n",
    "\n",
    "        combined_loss = (1 - self.alpha) * wce_loss + self.alpha * dice\n",
    "\n",
    "        precision = self.val_precision(y_hat, y)\n",
    "        recall = self.val_recall(y_hat, y)\n",
    "        iou = self.val_iou(y_hat, y)\n",
    "        loss_f1 = self.val_f1(y_hat, y)\n",
    "\n",
    "        self.log('val_precision', precision)\n",
    "        self.log('val_recall', recall)\n",
    "        self.log('val_wce', wce_loss)\n",
    "        self.log('val_dice', dice)\n",
    "        self.log('val_iou', iou)\n",
    "        self.log('val_f1', loss_f1)\n",
    "        self.log('val_loss', combined_loss)\n",
    "\n",
    "        if self.current_epoch % 10 == 0:\n",
    "            x = (x - x.min()) / (x.max() - x.min())\n",
    "            x = x[:, 0:3]\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            y_hat = (y_hat > 0.5).float()\n",
    "\n",
    "            class_labels = {0: \"no landslide\", 1: \"landslide\"}  # Define class_labels here\n",
    "\n",
    "            self.logger.experiment.log({\n",
    "                \"image\": wandb.Image(x[0].cpu().detach().numpy(), masks={ \n",
    "                    \"predictions\": {\n",
    "                        \"mask_data\": y_hat[0][0].cpu().detach().numpy(),\n",
    "                        \"class_labels\": class_labels\n",
    "                    },\n",
    "                    \"ground_truth\": {\n",
    "                        \"mask_data\": y[0][0].cpu().detach().numpy(),\n",
    "                        \"class_labels\": class_labels\n",
    "                    }\n",
    "                })\n",
    "            })\n",
    "        return {'val_loss': combined_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs=3, middles=64, outs=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, middles, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(middles, outs, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(outs)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.bn(self.conv2(x)))\n",
    "        return self.pool(x), x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.en1 = Block(in_channels, 64, 64)\n",
    "        self.en2 = Block(64, 128, 128)\n",
    "        self.en3 = Block(128, 256, 256)\n",
    "        self.en4 = Block(256, 512, 512)\n",
    "        self.en5 = Block(512, 1024, 512)\n",
    "\n",
    "        self.upsample4 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
    "        self.de4 = Block(1024, 512, 256)\n",
    "\n",
    "        self.upsample3 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
    "        self.de3 = Block(512, 256, 128)\n",
    "\n",
    "        self.upsample2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
    "        self.de2 = Block(256, 128, 64)\n",
    "\n",
    "        self.upsample1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.de1 = Block(128, 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, e1 = self.en1(x)\n",
    "        x, e2 = self.en2(x)\n",
    "        x, e3 = self.en3(x)\n",
    "        x, e4 = self.en4(x)\n",
    "        _, x = self.en5(x)\n",
    "\n",
    "        x = self.upsample4(x)\n",
    "        x = torch.cat([x, e4], dim=1)\n",
    "        _, x = self.de4(x)\n",
    "\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat([x, e3], dim=1)\n",
    "        _, x = self.de3(x)\n",
    "\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([x, e2], dim=1)\n",
    "        _, x = self.de2(x)\n",
    "\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat([x, e1], dim=1)\n",
    "        _, x = self.de1(x)\n",
    "\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def dice_loss(y_hat, y):\n",
    "    smooth = 1e-6\n",
    "    y_hat = y_hat.view(-1)\n",
    "    y = y.view(-1)\n",
    "    intersection = (y_hat * y).sum()\n",
    "    union = y_hat.sum() + y.sum()\n",
    "    dice = (2 * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:14:10.211783Z",
     "iopub.status.busy": "2025-02-19T18:14:10.211439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88073/1941494101.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images loaded. Check the dataset path or file structure.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m):  \u001b[38;5;66;03m# Make sure the batch contains at least 10 images\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         img \u001b[38;5;241m=\u001b[39m all_images[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     64\u001b[0m         img \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m-\u001b[39m img\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (img\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m img\u001b[38;5;241m.\u001b[39mmin())  \u001b[38;5;66;03m# Normalize the image to [0, 1] range\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `DatasetLandslide` and `LandslideModel` are defined in the train script\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"/home/hks/MOU/DeepLabV3+/final_model.pth\"\n",
    "\n",
    "# Initialize the model architecture and load weights\n",
    "model = LandslideModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Prepare the dataset and DataLoader\n",
    "data_path = \"/kaggle/input/landslide4sense/TrainData\"\n",
    "dataset = DatasetLandslide(data_path)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Prepare to store results\n",
    "all_images = []\n",
    "all_gt = []\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate over the DataLoader and make predictions\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        predictions = torch.sigmoid(predictions)  # Apply sigmoid if using BCEWithLogitsLoss\n",
    "        \n",
    "        # Move data to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        masks = masks.cpu()\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "        # Store results\n",
    "        all_images.append(images)\n",
    "        all_gt.append(masks)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "        # Only get one batch of predictions for visualization\n",
    "        if len(all_predictions) >= 1:\n",
    "            break\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "if len(all_images) == 0:\n",
    "    print(\"No images loaded. Check the dataset path or file structure.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(all_images[0])} images in the first batch.\")\n",
    "\n",
    "# Create a plot with the input image, ground truth, and prediction for each image\n",
    "fig, ax = plt.subplots(3, 10, figsize=(20, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    if i < len(all_images[0]):  # Make sure the batch contains at least 10 images\n",
    "        img = all_images[0][i].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize the image to [0, 1] range\n",
    "        \n",
    "        # Plot the input image (RGB channels)\n",
    "        ax[0, i].imshow(img[:, :, 1:4])  # Assuming RGB channels are 1, 2, 3\n",
    "        ax[0, i].set_title(\"Input Image\")\n",
    "\n",
    "        # Plot the ground truth mask with a color map\n",
    "        ax[1, i].imshow(all_gt[0][i][0].numpy(), cmap='viridis')\n",
    "        ax[1, i].set_title(\"Ground Truth\")\n",
    "\n",
    "        # Plot the prediction with a color map\n",
    "        ax[2, i].imshow(all_predictions[0][i][0] > 0.5, cmap='plasma')\n",
    "        ax[2, i].set_title(\"Prediction\")\n",
    "\n",
    "        # Remove axis\n",
    "        for j in range(3):\n",
    "            ax[j, i].set_xticks([])\n",
    "            ax[j, i].set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.02, wspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:14:17.098051Z",
     "iopub.status.busy": "2025-02-19T18:14:17.097750Z",
     "iopub.status.idle": "2025-02-19T18:14:19.774045Z",
     "shell.execute_reply": "2025-02-19T18:14:19.773097Z",
     "shell.execute_reply.started": "2025-02-19T18:14:17.098027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84976/4138273165.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/deeplabv3/pytorch/default/1/final_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the model architecture and load weights\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m LandslideModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Prepare the dataset and DataLoader\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/deeplabv3/pytorch/default/1/final_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `DatasetLandslide` and `LandslideModel` are defined in the train script\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"/kaggle/input/deeplabv3/pytorch/default/1/final_model.pth\"\n",
    "\n",
    "# Initialize the model architecture and load weights\n",
    "model = LandslideModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Prepare the dataset and DataLoader\n",
    "data_path = \"/kaggle/input/landslide4sense/TrainData\"\n",
    "dataset = DatasetLandslide(data_path)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Prepare to store results\n",
    "all_images = []\n",
    "all_gt = []\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate over the DataLoader and make predictions\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        predictions = torch.sigmoid(predictions)  # Apply sigmoid if using BCEWithLogitsLoss\n",
    "        \n",
    "        # Move data to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        masks = masks.cpu()\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "        # Store results\n",
    "        all_images.append(images)\n",
    "        all_gt.append(masks)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "        # Only get one batch of predictions for visualization\n",
    "        if len(all_predictions) >= 1:\n",
    "            break\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "if len(all_images) == 0:\n",
    "    print(\"No images loaded. Check the dataset path or file structure.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(all_images[0])} images in the first batch.\")\n",
    "\n",
    "# Create a plot with the input image, ground truth, and prediction for each image\n",
    "fig, ax = plt.subplots(3, 10, figsize=(20, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    if i < len(all_images[0]):  # Make sure the batch contains at least 10 images\n",
    "        img = all_images[0][i].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize the image to [0, 1] range\n",
    "        \n",
    "        # Plot the input image (RGB channels)\n",
    "        ax[0, i].imshow(img[:, :, 1:4])  # Assuming RGB channels are 1, 2, 3\n",
    "\n",
    "        # Plot the ground truth mask with a color map\n",
    "        ax[1, i].imshow(all_gt[0][i][0].numpy(), cmap='viridis')\n",
    "\n",
    "        # Plot the prediction with a color map\n",
    "        ax[2, i].imshow(all_predictions[0][i][0] > 0.5, cmap='plasma')\n",
    "\n",
    "        # Remove axis\n",
    "        for j in range(3):\n",
    "            ax[j, i].set_xticks([])\n",
    "            ax[j, i].set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.02, wspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88073/2980946415.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in HDF5 file: ['img']\n",
      "Loaded image shape: (128, 128, 14), Data type: float64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Wrong input shape height=128, width=14. Expected image height and width divisible by 16. Consider pad your images to shape (128, 16).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Forward pass (prediction)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(prediction)  \u001b[38;5;66;03m# Apply sigmoid if using BCEWithLogitsLoss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Move data to CPU for visualization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mLandslideModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36msmp_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/segmentation_models_pytorch/base/model.py:46\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_divisible_input_shape:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     49\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/segmentation_models_pytorch/base/model.py:37\u001b[0m, in \u001b[0;36mSegmentationModel.check_input_shape\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m new_h \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     28\u001b[0m     (h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m output_stride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m output_stride\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m%\u001b[39m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m h\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m new_w \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m     (w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m output_stride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m output_stride\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m%\u001b[39m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m w\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong input shape height=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, width=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected image height and width \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivisible by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_stride\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider pad your images to shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Wrong input shape height=128, width=14. Expected image height and width divisible by 16. Consider pad your images to shape (128, 16)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"/home/hks/MOU/DeepLabV3+/final_model.pth\"\n",
    "\n",
    "# Initialize the model architecture and load weights\n",
    "model = LandslideModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Path to the HDF5 file\n",
    "h5_file_path = \"/home/hks/MOU/image_10.h5\"\n",
    "\n",
    "# Check if the HDF5 file exists\n",
    "if not os.path.exists(h5_file_path):\n",
    "    print(f\"Error: HDF5 file {h5_file_path} does not exist!\")\n",
    "    exit()\n",
    "\n",
    "# Load the image from HDF5\n",
    "with h5py.File(h5_file_path, \"r\") as h5f:\n",
    "    dataset_keys = list(h5f.keys())  # Get dataset names inside the file\n",
    "    print(f\"Datasets in HDF5 file: {dataset_keys}\")\n",
    "\n",
    "    # Assuming image is stored under the first key (modify if needed)\n",
    "    img = np.array(h5f[dataset_keys[0]])  # Extract the image array\n",
    "\n",
    "# Check the image shape\n",
    "print(f\"Loaded image shape: {img.shape}, Data type: {img.dtype}\")\n",
    "\n",
    "# Convert to PyTorch tensor and add batch dimension\n",
    "image_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).to(device)  # Shape: (1, C, H, W)\n",
    "\n",
    "# Forward pass (prediction)\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor)\n",
    "    prediction = torch.sigmoid(prediction)  # Apply sigmoid if using BCEWithLogitsLoss\n",
    "\n",
    "# Move data to CPU for visualization\n",
    "image_tensor = image_tensor.cpu()\n",
    "prediction = prediction.cpu().numpy()\n",
    "\n",
    "# Prepare image for visualization\n",
    "img = img.transpose(1, 2, 0)  # Convert (C, H, W) -> (H, W, C)\n",
    "img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Plot the input image and prediction\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot the input image (first three channels as RGB)\n",
    "ax[0].imshow(img[:, :, :3])  # Displaying first 3 channels\n",
    "ax[0].set_title(\"Input Image\")\n",
    "\n",
    "# Plot the prediction mask\n",
    "ax[1].imshow(prediction[0][0] > 0.5, cmap='plasma')\n",
    "ax[1].set_title(\"Prediction\")\n",
    "\n",
    "# Remove axis ticks\n",
    "for a in ax:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4160916,
     "sourceId": 7194910,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 246766,
     "modelInstanceId": 225043,
     "sourceId": 263136,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
