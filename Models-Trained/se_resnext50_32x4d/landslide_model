digraph {
	graph [size="250.95,250.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140616712599360 [label="
 (1, 1, 128, 128)" fillcolor=darkolivegreen1]
	140616594173056 [label=ConvolutionBackward0]
	140616594173392 -> 140616594173056
	140616594173392 [label=ReluBackward0]
	140616594173104 -> 140616594173392
	140616594173104 [label=CudnnBatchNormBackward0]
	140616594173008 -> 140616594173104
	140616594173008 [label=ConvolutionBackward0]
	140616594173584 -> 140616594173008
	140616594173584 [label=ReluBackward0]
	140616594174208 -> 140616594173584
	140616594174208 [label=CudnnBatchNormBackward0]
	140616594174256 -> 140616594174208
	140616594174256 [label=ConvolutionBackward0]
	140616594174544 -> 140616594174256
	140616594174544 [label=UpsampleNearest2DBackward0]
	140616594174448 -> 140616594174544
	140616594174448 [label=ReluBackward0]
	140616594175168 -> 140616594174448
	140616594175168 [label=CudnnBatchNormBackward0]
	140616594175024 -> 140616594175168
	140616594175024 [label=ConvolutionBackward0]
	140616594177328 -> 140616594175024
	140616594177328 [label=ReluBackward0]
	140616594177664 -> 140616594177328
	140616594177664 [label=CudnnBatchNormBackward0]
	140616594177712 -> 140616594177664
	140616594177712 [label=ConvolutionBackward0]
	140616594177904 -> 140616594177712
	140616594177904 [label=CatBackward0]
	140616594168064 -> 140616594177904
	140616594168064 [label=UpsampleNearest2DBackward0]
	140616594170224 -> 140616594168064
	140616594170224 [label=ReluBackward0]
	140616594177040 -> 140616594170224
	140616594177040 [label=CudnnBatchNormBackward0]
	140616594176848 -> 140616594177040
	140616594176848 [label=ConvolutionBackward0]
	140616594176272 -> 140616594176848
	140616594176272 [label=ReluBackward0]
	140616594175408 -> 140616594176272
	140616594175408 [label=CudnnBatchNormBackward0]
	140616594175600 -> 140616594175408
	140616594175600 [label=ConvolutionBackward0]
	140616594166384 -> 140616594175600
	140616594166384 [label=CatBackward0]
	140616594177088 -> 140616594166384
	140616594177088 [label=UpsampleNearest2DBackward0]
	140616594176512 -> 140616594177088
	140616594176512 [label=ReluBackward0]
	140616608345792 -> 140616594176512
	140616608345792 [label=CudnnBatchNormBackward0]
	140617065896288 -> 140616608345792
	140617065896288 [label=ConvolutionBackward0]
	140616594732560 -> 140617065896288
	140616594732560 [label=ReluBackward0]
	140616594731648 -> 140616594732560
	140616594731648 [label=CudnnBatchNormBackward0]
	140616594731024 -> 140616594731648
	140616594731024 [label=ConvolutionBackward0]
	140616594729872 -> 140616594731024
	140616594729872 [label=CatBackward0]
	140616594730784 -> 140616594729872
	140616594730784 [label=UpsampleNearest2DBackward0]
	140616594729680 -> 140616594730784
	140616594729680 [label=ReluBackward0]
	140616594729440 -> 140616594729680
	140616594729440 [label=CudnnBatchNormBackward0]
	140616594729056 -> 140616594729440
	140616594729056 [label=ConvolutionBackward0]
	140616594728480 -> 140616594729056
	140616594728480 [label=ReluBackward0]
	140616594728576 -> 140616594728480
	140616594728576 [label=CudnnBatchNormBackward0]
	140616594728192 -> 140616594728576
	140616594728192 [label=ConvolutionBackward0]
	140616594723632 -> 140616594728192
	140616594723632 [label=CatBackward0]
	140616594720128 -> 140616594723632
	140616594720128 [label=UpsampleNearest2DBackward0]
	140616594720224 -> 140616594720128
	140616594720224 [label=ReluBackward0]
	140616594719840 -> 140616594720224
	140616594719840 [label=AddBackward0]
	140616594719312 -> 140616594719840
	140616594719312 [label=MulBackward0]
	140616594722528 -> 140616594719312
	140616594722528 [label=CudnnBatchNormBackward0]
	140616594722480 -> 140616594722528
	140616594722480 [label=ConvolutionBackward0]
	140616594721856 -> 140616594722480
	140616594721856 [label=ReluBackward0]
	140616594720992 -> 140616594721856
	140616594720992 [label=CudnnBatchNormBackward0]
	140616594720368 -> 140616594720992
	140616594720368 [label=ConvolutionBackward0]
	140616594734912 -> 140616594720368
	140616594734912 [label=ReluBackward0]
	140616594734864 -> 140616594734912
	140616594734864 [label=CudnnBatchNormBackward0]
	140616594734624 -> 140616594734864
	140616594734624 [label=ConvolutionBackward0]
	140616594719024 -> 140616594734624
	140616594719024 [label=ReluBackward0]
	140616594733664 -> 140616594719024
	140616594733664 [label=AddBackward0]
	140616594733040 -> 140616594733664
	140616594733040 [label=MulBackward0]
	140616594726704 -> 140616594733040
	140616594726704 [label=CudnnBatchNormBackward0]
	140616594727040 -> 140616594726704
	140616594727040 [label=ConvolutionBackward0]
	140616594726464 -> 140616594727040
	140616594726464 [label=ReluBackward0]
	140616594725312 -> 140616594726464
	140616594725312 [label=CudnnBatchNormBackward0]
	140616594725120 -> 140616594725312
	140616594725120 [label=ConvolutionBackward0]
	140616594724544 -> 140616594725120
	140616594724544 [label=ReluBackward0]
	140616594724496 -> 140616594724544
	140616594724496 [label=CudnnBatchNormBackward0]
	140616594724256 -> 140616594724496
	140616594724256 [label=ConvolutionBackward0]
	140616594727712 -> 140616594724256
	140616594727712 [label=ReluBackward0]
	140616594731456 -> 140616594727712
	140616594731456 [label=AddBackward0]
	140616594730880 -> 140616594731456
	140616594730880 [label=MulBackward0]
	140616594729104 -> 140616594730880
	140616594729104 [label=CudnnBatchNormBackward0]
	140616594728432 -> 140616594729104
	140616594728432 [label=ConvolutionBackward0]
	140616594723536 -> 140616594728432
	140616594723536 [label=ReluBackward0]
	140616594719216 -> 140616594723536
	140616594719216 [label=CudnnBatchNormBackward0]
	140616594722720 -> 140616594719216
	140616594722720 [label=ConvolutionBackward0]
	140616594721328 -> 140616594722720
	140616594721328 [label=ReluBackward0]
	140616594721376 -> 140616594721328
	140616594721376 [label=CudnnBatchNormBackward0]
	140616594720800 -> 140616594721376
	140616594720800 [label=ConvolutionBackward0]
	140616594723248 -> 140616594720800
	140616594723248 [label=ReluBackward0]
	140616594733472 -> 140616594723248
	140616594733472 [label=AddBackward0]
	140616594727280 -> 140616594733472
	140616594727280 [label=MulBackward0]
	140616594725888 -> 140616594727280
	140616594725888 [label=CudnnBatchNormBackward0]
	140616594724976 -> 140616594725888
	140616594724976 [label=ConvolutionBackward0]
	140616594722384 -> 140616594724976
	140616594722384 [label=ReluBackward0]
	140616594702048 -> 140616594722384
	140616594702048 [label=CudnnBatchNormBackward0]
	140616594695328 -> 140616594702048
	140616594695328 [label=ConvolutionBackward0]
	140616594694512 -> 140616594695328
	140616594694512 [label=ReluBackward0]
	140616594691632 -> 140616594694512
	140616594691632 [label=CudnnBatchNormBackward0]
	140616594691392 -> 140616594691632
	140616594691392 [label=ConvolutionBackward0]
	140616594727088 -> 140616594691392
	140616594727088 [label=ReluBackward0]
	140616594687600 -> 140616594727088
	140616594687600 [label=AddBackward0]
	140616594687216 -> 140616594687600
	140616594687216 [label=MulBackward0]
	140616594701712 -> 140616594687216
	140616594701712 [label=CudnnBatchNormBackward0]
	140616594701376 -> 140616594701712
	140616594701376 [label=ConvolutionBackward0]
	140616594700800 -> 140616594701376
	140616594700800 [label=ReluBackward0]
	140616594700320 -> 140616594700800
	140616594700320 [label=CudnnBatchNormBackward0]
	140616594699936 -> 140616594700320
	140616594699936 [label=ConvolutionBackward0]
	140616594699360 -> 140616594699936
	140616594699360 [label=ReluBackward0]
	140616594693936 -> 140616594699360
	140616594693936 [label=CudnnBatchNormBackward0]
	140616594693696 -> 140616594693936
	140616594693696 [label=ConvolutionBackward0]
	140616594687552 -> 140616594693696
	140616594687552 [label=ReluBackward0]
	140616594692736 -> 140616594687552
	140616594692736 [label=AddBackward0]
	140616594691824 -> 140616594692736
	140616594691824 [label=MulBackward0]
	140616594690480 -> 140616594691824
	140616594690480 [label=CudnnBatchNormBackward0]
	140616594690192 -> 140616594690480
	140616594690192 [label=ConvolutionBackward0]
	140616594689568 -> 140616594690192
	140616594689568 [label=ReluBackward0]
	140616594688656 -> 140616594689568
	140616594688656 [label=CudnnBatchNormBackward0]
	140616594688704 -> 140616594688656
	140616594688704 [label=ConvolutionBackward0]
	140616594686736 -> 140616594688704
	140616594686736 [label=ReluBackward0]
	140616594686400 -> 140616594686736
	140616594686400 [label=CudnnBatchNormBackward0]
	140616594699120 -> 140616594686400
	140616594699120 [label=ConvolutionBackward0]
	140616594692448 -> 140616594699120
	140616594692448 [label=ReluBackward0]
	140616594698160 -> 140616594692448
	140616594698160 [label=AddBackward0]
	140616594697968 -> 140616594698160
	140616594697968 [label=MulBackward0]
	140616594696720 -> 140616594697968
	140616594696720 [label=CudnnBatchNormBackward0]
	140616594697152 -> 140616594696720
	140616594697152 [label=ConvolutionBackward0]
	140616594696144 -> 140616594697152
	140616594696144 [label=ReluBackward0]
	140616594695904 -> 140616594696144
	140616594695904 [label=CudnnBatchNormBackward0]
	140616594695376 -> 140616594695904
	140616594695376 [label=ConvolutionBackward0]
	140616594691584 -> 140616594695376
	140616594691584 [label=ReluBackward0]
	140616594690960 -> 140616594691584
	140616594690960 [label=CudnnBatchNormBackward0]
	140616594687936 -> 140616594690960
	140616594687936 [label=ConvolutionBackward0]
	140616594697872 -> 140616594687936
	140616594697872 [label=ReluBackward0]
	140616594701184 -> 140616594697872
	140616594701184 [label=AddBackward0]
	140616594700608 -> 140616594701184
	140616594700608 [label=MulBackward0]
	140616594693888 -> 140616594700608
	140616594693888 [label=CudnnBatchNormBackward0]
	140616594693264 -> 140616594693888
	140616594693264 [label=ConvolutionBackward0]
	140616594692256 -> 140616594693264
	140616594692256 [label=ReluBackward0]
	140616594689328 -> 140616594692256
	140616594689328 [label=CudnnBatchNormBackward0]
	140616594688992 -> 140616594689328
	140616594688992 [label=ConvolutionBackward0]
	140616594686208 -> 140616594688992
	140616594686208 [label=ReluBackward0]
	140616594686112 -> 140616594686208
	140616594686112 [label=CudnnBatchNormBackward0]
	140616594698784 -> 140616594686112
	140616594698784 [label=ConvolutionBackward0]
	140616594699984 -> 140616594698784
	140616594699984 [label=ReluBackward0]
	140616594696528 -> 140616594699984
	140616594696528 [label=AddBackward0]
	140616594695952 -> 140616594696528
	140616594695952 [label=MulBackward0]
	140616594819296 -> 140616594695952
	140616594819296 [label=CudnnBatchNormBackward0]
	140616594820112 -> 140616594819296
	140616594820112 [label=ConvolutionBackward0]
	140616594820400 -> 140616594820112
	140616594820400 [label=ReluBackward0]
	140616594818576 -> 140616594820400
	140616594818576 [label=CudnnBatchNormBackward0]
	140616594819776 -> 140616594818576
	140616594819776 [label=ConvolutionBackward0]
	140616594817472 -> 140616594819776
	140616594817472 [label=ReluBackward0]
	140616594817712 -> 140616594817472
	140616594817712 [label=CudnnBatchNormBackward0]
	140616594817616 -> 140616594817712
	140616594817616 [label=ConvolutionBackward0]
	140616594730496 -> 140616594817616
	140616594730496 [label=ReluBackward0]
	140616594821024 -> 140616594730496
	140616594821024 [label=AddBackward0]
	140616594820928 -> 140616594821024
	140616594820928 [label=MulBackward0]
	140616594821648 -> 140616594820928
	140616594821648 [label=CudnnBatchNormBackward0]
	140616594821792 -> 140616594821648
	140616594821792 [label=ConvolutionBackward0]
	140616594830144 -> 140616594821792
	140616594830144 [label=ReluBackward0]
	140616594828368 -> 140616594830144
	140616594828368 [label=CudnnBatchNormBackward0]
	140616594829184 -> 140616594828368
	140616594829184 [label=ConvolutionBackward0]
	140616594833072 -> 140616594829184
	140616594833072 [label=ReluBackward0]
	140616594833168 -> 140616594833072
	140616594833168 [label=CudnnBatchNormBackward0]
	140616594833312 -> 140616594833168
	140616594833312 [label=ConvolutionBackward0]
	140616594820976 -> 140616594833312
	140616594820976 [label=ReluBackward0]
	140616594828896 -> 140616594820976
	140616594828896 [label=AddBackward0]
	140616594832256 -> 140616594828896
	140616594832256 [label=MulBackward0]
	140616594831920 -> 140616594832256
	140616594831920 [label=CudnnBatchNormBackward0]
	140616594829040 -> 140616594831920
	140616594829040 [label=ConvolutionBackward0]
	140616594831536 -> 140616594829040
	140616594831536 [label=ReluBackward0]
	140616594830576 -> 140616594831536
	140616594830576 [label=CudnnBatchNormBackward0]
	140616594830672 -> 140616594830576
	140616594830672 [label=ConvolutionBackward0]
	140616594830720 -> 140616594830672
	140616594830720 [label=ReluBackward0]
	140616594831104 -> 140616594830720
	140616594831104 [label=CudnnBatchNormBackward0]
	140616594830960 -> 140616594831104
	140616594830960 [label=ConvolutionBackward0]
	140616594832352 -> 140616594830960
	140616594832352 [label=ReluBackward0]
	140616594830768 -> 140616594832352
	140616594830768 [label=AddBackward0]
	140616594830528 -> 140616594830768
	140616594830528 [label=MulBackward0]
	140616594828464 -> 140616594830528
	140616594828464 [label=CudnnBatchNormBackward0]
	140616594829760 -> 140616594828464
	140616594829760 [label=ConvolutionBackward0]
	140616594830432 -> 140616594829760
	140616594830432 [label=ReluBackward0]
	140616594830048 -> 140616594830432
	140616594830048 [label=CudnnBatchNormBackward0]
	140616594819440 -> 140616594830048
	140616594819440 [label=ConvolutionBackward0]
	140616594819632 -> 140616594819440
	140616594819632 [label=ReluBackward0]
	140617843544784 -> 140616594819632
	140617843544784 [label=CudnnBatchNormBackward0]
	140616594545712 -> 140617843544784
	140616594545712 [label=ConvolutionBackward0]
	140616594831008 -> 140616594545712
	140616594831008 [label=ReluBackward0]
	140616594546624 -> 140616594831008
	140616594546624 [label=AddBackward0]
	140616594546816 -> 140616594546624
	140616594546816 [label=MulBackward0]
	140616594547440 -> 140616594546816
	140616594547440 [label=CudnnBatchNormBackward0]
	140616594547920 -> 140616594547440
	140616594547920 [label=ConvolutionBackward0]
	140616594548496 -> 140616594547920
	140616594548496 [label=ReluBackward0]
	140616594548928 -> 140616594548496
	140616594548928 [label=CudnnBatchNormBackward0]
	140616594549168 -> 140616594548928
	140616594549168 [label=ConvolutionBackward0]
	140616594549696 -> 140616594549168
	140616594549696 [label=ReluBackward0]
	140616594550512 -> 140616594549696
	140616594550512 [label=CudnnBatchNormBackward0]
	140616594550560 -> 140616594550512
	140616594550560 [label=ConvolutionBackward0]
	140616594176464 -> 140616594550560
	140616594176464 [label=ReluBackward0]
	140616594551808 -> 140616594176464
	140616594551808 [label=AddBackward0]
	140616594552000 -> 140616594551808
	140616594552000 [label=MulBackward0]
	140616594552288 -> 140616594552000
	140616594552288 [label=CudnnBatchNormBackward0]
	140616594553104 -> 140616594552288
	140616594553104 [label=ConvolutionBackward0]
	140616594553968 -> 140616594553104
	140616594553968 [label=ReluBackward0]
	140616594554256 -> 140616594553968
	140616594554256 [label=CudnnBatchNormBackward0]
	140616594554064 -> 140616594554256
	140616594554064 [label=ConvolutionBackward0]
	140616594545136 -> 140616594554064
	140616594545136 [label=ReluBackward0]
	140616594544464 -> 140616594545136
	140616594544464 [label=CudnnBatchNormBackward0]
	140616594544272 -> 140616594544464
	140616594544272 [label=ConvolutionBackward0]
	140616594551712 -> 140616594544272
	140616594551712 [label=ReluBackward0]
	140616594540864 -> 140616594551712
	140616594540864 [label=AddBackward0]
	140616594540480 -> 140616594540864
	140616594540480 [label=MulBackward0]
	140616594543696 -> 140616594540480
	140616594543696 [label=CudnnBatchNormBackward0]
	140616594543168 -> 140616594543696
	140616594543168 [label=ConvolutionBackward0]
	140616594542592 -> 140616594543168
	140616594542592 [label=ReluBackward0]
	140616594541584 -> 140616594542592
	140616594541584 [label=CudnnBatchNormBackward0]
	140616594541680 -> 140616594541584
	140616594541680 [label=ConvolutionBackward0]
	140616594539952 -> 140616594541680
	140616594539952 [label=ReluBackward0]
	140616594539280 -> 140616594539952
	140616594539280 [label=CudnnBatchNormBackward0]
	140616594539088 -> 140616594539280
	140616594539088 [label=ConvolutionBackward0]
	140616594540768 -> 140616594539088
	140616594540768 [label=ReluBackward0]
	140616594543648 -> 140616594540768
	140616594543648 [label=AddBackward0]
	140616594553728 -> 140616594543648
	140616594553728 [label=MulBackward0]
	140616594544512 -> 140616594553728
	140616594544512 [label=CudnnBatchNormBackward0]
	140616594554544 -> 140616594544512
	140616594554544 [label=ConvolutionBackward0]
	140616594552048 -> 140616594554544
	140616594552048 [label=ReluBackward0]
	140616594552624 -> 140616594552048
	140616594552624 [label=CudnnBatchNormBackward0]
	140616594550944 -> 140616594552624
	140616594550944 [label=ConvolutionBackward0]
	140616594547968 -> 140616594550944
	140616594547968 [label=ReluBackward0]
	140616594548784 -> 140616594547968
	140616594548784 [label=CudnnBatchNormBackward0]
	140616594545904 -> 140616594548784
	140616594545904 [label=ConvolutionBackward0]
	140616594548208 -> 140616594545904
	140616594548208 [label=MaxPool2DWithIndicesBackward0]
	140616594175360 -> 140616594548208
	140616594175360 [label=ReluBackward0]
	140616594542256 -> 140616594175360
	140616594542256 [label=CudnnBatchNormBackward0]
	140616594541632 -> 140616594542256
	140616594541632 [label=ConvolutionBackward0]
	140616594539328 -> 140616594541632
	140616594250688 [label="model.model.encoder.layer0.conv1.weight
 (64, 14, 7, 7)" fillcolor=lightblue]
	140616594250688 -> 140616594539328
	140616594539328 [label=AccumulateGrad]
	140616594540144 -> 140616594542256
	140617870292080 [label="model.model.encoder.layer0.bn1.weight
 (64)" fillcolor=lightblue]
	140617870292080 -> 140616594540144
	140616594540144 [label=AccumulateGrad]
	140616594543120 -> 140616594542256
	140617870292000 [label="model.model.encoder.layer0.bn1.bias
 (64)" fillcolor=lightblue]
	140617870292000 -> 140616594543120
	140616594543120 [label=AccumulateGrad]
	140616594546000 -> 140616594545904
	140617870290240 [label="model.model.encoder.layer1.0.conv1.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140617870290240 -> 140616594546000
	140616594546000 [label=AccumulateGrad]
	140616594549120 -> 140616594548784
	140617870290160 [label="model.model.encoder.layer1.0.bn1.weight
 (128)" fillcolor=lightblue]
	140617870290160 -> 140616594549120
	140616594549120 [label=AccumulateGrad]
	140616594548544 -> 140616594548784
	140617870290320 [label="model.model.encoder.layer1.0.bn1.bias
 (128)" fillcolor=lightblue]
	140617870290320 -> 140616594548544
	140616594548544 [label=AccumulateGrad]
	140616594551088 -> 140616594550944
	140617870289200 [label="model.model.encoder.layer1.0.conv2.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	140617870289200 -> 140616594551088
	140616594551088 [label=AccumulateGrad]
	140616594550368 -> 140616594552624
	140617870289520 [label="model.model.encoder.layer1.0.bn2.weight
 (128)" fillcolor=lightblue]
	140617870289520 -> 140616594550368
	140616594550368 [label=AccumulateGrad]
	140616594552960 -> 140616594552624
	140617870289120 [label="model.model.encoder.layer1.0.bn2.bias
 (128)" fillcolor=lightblue]
	140617870289120 -> 140616594552960
	140616594552960 [label=AccumulateGrad]
	140616594551136 -> 140616594554544
	140617870288320 [label="model.model.encoder.layer1.0.conv3.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140617870288320 -> 140616594551136
	140616594551136 [label=AccumulateGrad]
	140616594554400 -> 140616594544512
	140617870288480 [label="model.model.encoder.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	140617870288480 -> 140616594554400
	140616594554400 [label=AccumulateGrad]
	140616594554832 -> 140616594544512
	140617870288160 [label="model.model.encoder.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	140617870288160 -> 140616594554832
	140616594554832 [label=AccumulateGrad]
	140616594544896 -> 140616594553728
	140616594544896 [label=SigmoidBackward0]
	140616594550032 -> 140616594544896
	140616594550032 [label=ConvolutionBackward0]
	140616594552096 -> 140616594550032
	140616594552096 [label=ReluBackward0]
	140616594538992 -> 140616594552096
	140616594538992 [label=ConvolutionBackward0]
	140616594542016 -> 140616594538992
	140616594542016 [label=MeanBackward1]
	140616594544512 -> 140616594542016
	140616594541872 -> 140616594538992
	140617870288000 [label="model.model.encoder.layer1.0.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	140617870288000 -> 140616594541872
	140616594541872 [label=AccumulateGrad]
	140616594547632 -> 140616594538992
	140617870287360 [label="model.model.encoder.layer1.0.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	140617870287360 -> 140616594547632
	140616594547632 [label=AccumulateGrad]
	140616594550272 -> 140616594550032
	140617870287440 [label="model.model.encoder.layer1.0.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	140617870287440 -> 140616594550272
	140616594550272 [label=AccumulateGrad]
	140616594553152 -> 140616594550032
	140617870287120 [label="model.model.encoder.layer1.0.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	140617870287120 -> 140616594553152
	140616594553152 [label=AccumulateGrad]
	140616594554304 -> 140616594543648
	140616594554304 [label=CudnnBatchNormBackward0]
	140616594547392 -> 140616594554304
	140616594547392 [label=ConvolutionBackward0]
	140616594548208 -> 140616594547392
	140616594539904 -> 140616594547392
	140617870291200 [label="model.model.encoder.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140617870291200 -> 140616594539904
	140616594539904 [label=AccumulateGrad]
	140616594553824 -> 140616594554304
	140617870291680 [label="model.model.encoder.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140617870291680 -> 140616594553824
	140616594553824 [label=AccumulateGrad]
	140616594541296 -> 140616594554304
	140617870291040 [label="model.model.encoder.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140617870291040 -> 140616594541296
	140616594541296 [label=AccumulateGrad]
	140616594541152 -> 140616594539088
	140617870287200 [label="model.model.encoder.layer1.1.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140617870287200 -> 140616594541152
	140616594541152 [label=AccumulateGrad]
	140616594539376 -> 140616594539280
	140617870286720 [label="model.model.encoder.layer1.1.bn1.weight
 (128)" fillcolor=lightblue]
	140617870286720 -> 140616594539376
	140616594539376 [label=AccumulateGrad]
	140616594539664 -> 140616594539280
	140617870286480 [label="model.model.encoder.layer1.1.bn1.bias
 (128)" fillcolor=lightblue]
	140617870286480 -> 140616594539664
	140616594539664 [label=AccumulateGrad]
	140616594539568 -> 140616594541680
	140617870286160 [label="model.model.encoder.layer1.1.conv2.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	140617870286160 -> 140616594539568
	140616594539568 [label=AccumulateGrad]
	140616594539856 -> 140616594541584
	140617870285680 [label="model.model.encoder.layer1.1.bn2.weight
 (128)" fillcolor=lightblue]
	140617870285680 -> 140616594539856
	140616594539856 [label=AccumulateGrad]
	140616594542496 -> 140616594541584
	140617870285520 [label="model.model.encoder.layer1.1.bn2.bias
 (128)" fillcolor=lightblue]
	140617870285520 -> 140616594542496
	140616594542496 [label=AccumulateGrad]
	140616594542880 -> 140616594543168
	140617870284640 [label="model.model.encoder.layer1.1.conv3.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140617870284640 -> 140616594542880
	140616594542880 [label=AccumulateGrad]
	140616594543456 -> 140616594543696
	140617870284800 [label="model.model.encoder.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	140617870284800 -> 140616594543456
	140616594543456 [label=AccumulateGrad]
	140616594543408 -> 140616594543696
	140617870284480 [label="model.model.encoder.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	140617870284480 -> 140616594543408
	140616594543408 [label=AccumulateGrad]
	140616594543312 -> 140616594540480
	140616594543312 [label=SigmoidBackward0]
	140616594540288 -> 140616594543312
	140616594540288 [label=ConvolutionBackward0]
	140616594542208 -> 140616594540288
	140616594542208 [label=ReluBackward0]
	140616594549648 -> 140616594542208
	140616594549648 [label=ConvolutionBackward0]
	140616594543024 -> 140616594549648
	140616594543024 [label=MeanBackward1]
	140616594543696 -> 140616594543024
	140616594538800 -> 140616594549648
	140617870283680 [label="model.model.encoder.layer1.1.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	140617870283680 -> 140616594538800
	140616594538800 [label=AccumulateGrad]
	140616594539136 -> 140616594549648
	140617870283600 [label="model.model.encoder.layer1.1.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	140617870283600 -> 140616594539136
	140616594539136 [label=AccumulateGrad]
	140616594540240 -> 140616594540288
	140617870284320 [label="model.model.encoder.layer1.1.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	140617870284320 -> 140616594540240
	140616594540240 [label=AccumulateGrad]
	140616594543072 -> 140616594540288
	140617870283760 [label="model.model.encoder.layer1.1.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	140617870283760 -> 140616594543072
	140616594543072 [label=AccumulateGrad]
	140616594540768 -> 140616594540864
	140616594541056 -> 140616594544272
	140617870283520 [label="model.model.encoder.layer1.2.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140617870283520 -> 140616594541056
	140616594541056 [label=AccumulateGrad]
	140616594544560 -> 140616594544464
	140617870283040 [label="model.model.encoder.layer1.2.bn1.weight
 (128)" fillcolor=lightblue]
	140617870283040 -> 140616594544560
	140616594544560 [label=AccumulateGrad]
	140616594545088 -> 140616594544464
	140617870282800 [label="model.model.encoder.layer1.2.bn1.bias
 (128)" fillcolor=lightblue]
	140617870282800 -> 140616594545088
	140616594545088 [label=AccumulateGrad]
	140616594545040 -> 140616594554064
	140617870282480 [label="model.model.encoder.layer1.2.conv2.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	140617870282480 -> 140616594545040
	140616594545040 [label=AccumulateGrad]
	140616594553680 -> 140616594554256
	140617870282320 [label="model.model.encoder.layer1.2.bn2.weight
 (128)" fillcolor=lightblue]
	140617870282320 -> 140616594553680
	140616594553680 [label=AccumulateGrad]
	140616594554640 -> 140616594554256
	140617870282000 [label="model.model.encoder.layer1.2.bn2.bias
 (128)" fillcolor=lightblue]
	140617870282000 -> 140616594554640
	140616594554640 [label=AccumulateGrad]
	140616594553536 -> 140616594553104
	140617870281360 [label="model.model.encoder.layer1.2.conv3.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140617870281360 -> 140616594553536
	140616594553536 [label=AccumulateGrad]
	140616594552816 -> 140616594552288
	140617870281280 [label="model.model.encoder.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	140617870281280 -> 140616594552816
	140616594552816 [label=AccumulateGrad]
	140616594552576 -> 140616594552288
	140617870281440 [label="model.model.encoder.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	140617870281440 -> 140616594552576
	140616594552576 [label=AccumulateGrad]
	140616594552384 -> 140616594552000
	140616594552384 [label=SigmoidBackward0]
	140616594551952 -> 140616594552384
	140616594551952 [label=ConvolutionBackward0]
	140616594554592 -> 140616594551952
	140616594554592 [label=ReluBackward0]
	140616594543888 -> 140616594554592
	140616594543888 [label=ConvolutionBackward0]
	140616594540720 -> 140616594543888
	140616594540720 [label=MeanBackward1]
	140616594552288 -> 140616594540720
	140616594541440 -> 140616594543888
	140617870280640 [label="model.model.encoder.layer1.2.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	140617870280640 -> 140616594541440
	140616594541440 [label=AccumulateGrad]
	140616594544320 -> 140616594543888
	140617870280160 [label="model.model.encoder.layer1.2.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	140617870280160 -> 140616594544320
	140616594544320 [label=AccumulateGrad]
	140616594554016 -> 140616594551952
	140617870280400 [label="model.model.encoder.layer1.2.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	140617870280400 -> 140616594554016
	140616594554016 [label=AccumulateGrad]
	140616594553200 -> 140616594551952
	140617870279760 [label="model.model.encoder.layer1.2.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	140617870279760 -> 140616594553200
	140616594553200 [label=AccumulateGrad]
	140616594551712 -> 140616594551808
	140616594551472 -> 140616594550560
	140617870292880 [label="model.model.encoder.layer2.0.conv1.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	140617870292880 -> 140616594551472
	140616594551472 [label=AccumulateGrad]
	140616594550608 -> 140616594550512
	140617870293040 [label="model.model.encoder.layer2.0.bn1.weight
 (256)" fillcolor=lightblue]
	140617870293040 -> 140616594550608
	140616594550608 [label=AccumulateGrad]
	140616594549984 -> 140616594550512
	140617870293840 [label="model.model.encoder.layer2.0.bn1.bias
 (256)" fillcolor=lightblue]
	140617870293840 -> 140616594549984
	140616594549984 [label=AccumulateGrad]
	140616594549792 -> 140616594549168
	140617870293280 [label="model.model.encoder.layer2.0.conv2.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	140617870293280 -> 140616594549792
	140616594549792 [label=AccumulateGrad]
	140616594547344 -> 140616594548928
	140617870293120 [label="model.model.encoder.layer2.0.bn2.weight
 (256)" fillcolor=lightblue]
	140617870293120 -> 140616594547344
	140616594547344 [label=AccumulateGrad]
	140616594548592 -> 140616594548928
	140617870293520 [label="model.model.encoder.layer2.0.bn2.bias
 (256)" fillcolor=lightblue]
	140617870293520 -> 140616594548592
	140616594548592 [label=AccumulateGrad]
	140616594548352 -> 140616594547920
	140617060029184 [label="model.model.encoder.layer2.0.conv3.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140617060029184 -> 140616594548352
	140616594548352 [label=AccumulateGrad]
	140616594547776 -> 140616594547440
	140617060030384 [label="model.model.encoder.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	140617060030384 -> 140616594547776
	140616594547776 [label=AccumulateGrad]
	140616594547728 -> 140616594547440
	140621576994416 [label="model.model.encoder.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	140621576994416 -> 140616594547728
	140616594547728 [label=AccumulateGrad]
	140616594547056 -> 140616594546816
	140616594547056 [label=SigmoidBackward0]
	140616594549504 -> 140616594547056
	140616594549504 [label=ConvolutionBackward0]
	140616594548880 -> 140616594549504
	140616594548880 [label=ReluBackward0]
	140616594552240 -> 140616594548880
	140616594552240 [label=ConvolutionBackward0]
	140616594551520 -> 140616594552240
	140616594551520 [label=MeanBackward1]
	140616594547440 -> 140616594551520
	140616594550800 -> 140616594552240
	140616683878288 [label="model.model.encoder.layer2.0.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	140616683878288 -> 140616594550800
	140616594550800 [label=AccumulateGrad]
	140616594550224 -> 140616594552240
	140616683864608 [label="model.model.encoder.layer2.0.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	140616683864608 -> 140616594550224
	140616594550224 [label=AccumulateGrad]
	140616594549456 -> 140616594549504
	140616683864528 [label="model.model.encoder.layer2.0.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	140616683864528 -> 140616594549456
	140616594549456 [label=AccumulateGrad]
	140616594548016 -> 140616594549504
	140616683864448 [label="model.model.encoder.layer2.0.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	140616683864448 -> 140616594548016
	140616594548016 [label=AccumulateGrad]
	140616594546528 -> 140616594546624
	140616594546528 [label=CudnnBatchNormBackward0]
	140616594551184 -> 140616594546528
	140616594551184 [label=ConvolutionBackward0]
	140616594176464 -> 140616594551184
	140616594544752 -> 140616594551184
	140617870279840 [label="model.model.encoder.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140617870279840 -> 140616594544752
	140616594544752 [label=AccumulateGrad]
	140616594548304 -> 140616594546528
	140617870279360 [label="model.model.encoder.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140617870279360 -> 140616594548304
	140616594548304 [label=AccumulateGrad]
	140616594546480 -> 140616594546528
	140617870279120 [label="model.model.encoder.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140617870279120 -> 140616594546480
	140616594546480 [label=AccumulateGrad]
	140616594546288 -> 140616594545712
	140616683864768 [label="model.model.encoder.layer2.1.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140616683864768 -> 140616594546288
	140616594546288 [label=AccumulateGrad]
	140616594545424 -> 140617843544784
	140616683864288 [label="model.model.encoder.layer2.1.bn1.weight
 (256)" fillcolor=lightblue]
	140616683864288 -> 140616594545424
	140616594545424 [label=AccumulateGrad]
	140616594545328 -> 140617843544784
	140616683864128 [label="model.model.encoder.layer2.1.bn1.bias
 (256)" fillcolor=lightblue]
	140616683864128 -> 140616594545328
	140616594545328 [label=AccumulateGrad]
	140616594829376 -> 140616594819440
	140616683865008 [label="model.model.encoder.layer2.1.conv2.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	140616683865008 -> 140616594829376
	140616594829376 [label=AccumulateGrad]
	140616594829328 -> 140616594830048
	140616683864848 [label="model.model.encoder.layer2.1.bn2.weight
 (256)" fillcolor=lightblue]
	140616683864848 -> 140616594829328
	140616594829328 [label=AccumulateGrad]
	140616594830192 -> 140616594830048
	140616683865888 [label="model.model.encoder.layer2.1.bn2.bias
 (256)" fillcolor=lightblue]
	140616683865888 -> 140616594830192
	140616594830192 [label=AccumulateGrad]
	140616594830336 -> 140616594829760
	140616683866288 [label="model.model.encoder.layer2.1.conv3.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140616683866288 -> 140616594830336
	140616594830336 [label=AccumulateGrad]
	140616594828800 -> 140616594828464
	140616683865088 [label="model.model.encoder.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	140616683865088 -> 140616594828800
	140616594828800 [label=AccumulateGrad]
	140616594828656 -> 140616594828464
	140616683865648 [label="model.model.encoder.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	140616683865648 -> 140616594828656
	140616594828656 [label=AccumulateGrad]
	140616594819056 -> 140616594830528
	140616594819056 [label=SigmoidBackward0]
	140616594829856 -> 140616594819056
	140616594829856 [label=ConvolutionBackward0]
	140616594829808 -> 140616594829856
	140616594829808 [label=ReluBackward0]
	140616594549360 -> 140616594829808
	140616594549360 [label=ConvolutionBackward0]
	140616594546336 -> 140616594549360
	140616594546336 [label=MeanBackward1]
	140616594828464 -> 140616594546336
	140616594546048 -> 140616594549360
	140616683866928 [label="model.model.encoder.layer2.1.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	140616683866928 -> 140616594546048
	140616594546048 [label=AccumulateGrad]
	140616594545952 -> 140616594549360
	140616683867008 [label="model.model.encoder.layer2.1.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	140616683867008 -> 140616594545952
	140616594545952 [label=AccumulateGrad]
	140616594829664 -> 140616594829856
	140616683867168 [label="model.model.encoder.layer2.1.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	140616683867168 -> 140616594829664
	140616594829664 [label=AccumulateGrad]
	140617843544880 -> 140616594829856
	140616683866688 [label="model.model.encoder.layer2.1.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	140616683866688 -> 140617843544880
	140617843544880 [label=AccumulateGrad]
	140616594831008 -> 140616594830768
	140616594830864 -> 140616594830960
	140616683867328 [label="model.model.encoder.layer2.2.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140616683867328 -> 140616594830864
	140616594830864 [label=AccumulateGrad]
	140616594831392 -> 140616594831104
	140616683866768 [label="model.model.encoder.layer2.2.bn1.weight
 (256)" fillcolor=lightblue]
	140616683866768 -> 140616594831392
	140616594831392 [label=AccumulateGrad]
	140616594831296 -> 140616594831104
	140616683866848 [label="model.model.encoder.layer2.2.bn1.bias
 (256)" fillcolor=lightblue]
	140616683866848 -> 140616594831296
	140616594831296 [label=AccumulateGrad]
	140616594831200 -> 140616594830672
	140616683868128 [label="model.model.encoder.layer2.2.conv2.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	140616683868128 -> 140616594831200
	140616594831200 [label=AccumulateGrad]
	140616594830624 -> 140616594830576
	140616683867968 [label="model.model.encoder.layer2.2.bn2.weight
 (256)" fillcolor=lightblue]
	140616683867968 -> 140616594830624
	140616594830624 [label=AccumulateGrad]
	140616594831440 -> 140616594830576
	140616683868208 [label="model.model.encoder.layer2.2.bn2.bias
 (256)" fillcolor=lightblue]
	140616683868208 -> 140616594831440
	140616594831440 [label=AccumulateGrad]
	140616594831632 -> 140616594829040
	140616683867808 [label="model.model.encoder.layer2.2.conv3.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140616683867808 -> 140616594831632
	140616594831632 [label=AccumulateGrad]
	140616594832064 -> 140616594831920
	140616683868528 [label="model.model.encoder.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	140616683868528 -> 140616594832064
	140616594832064 [label=AccumulateGrad]
	140616594832112 -> 140616594831920
	140616683868608 [label="model.model.encoder.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	140616683868608 -> 140616594832112
	140616594832112 [label=AccumulateGrad]
	140616594831824 -> 140616594832256
	140616594831824 [label=SigmoidBackward0]
	140616594832400 -> 140616594831824
	140616594832400 [label=ConvolutionBackward0]
	140616594831584 -> 140616594832400
	140616594831584 [label=ReluBackward0]
	140616594830384 -> 140616594831584
	140616594830384 [label=ConvolutionBackward0]
	140616594830912 -> 140616594830384
	140616594830912 [label=MeanBackward1]
	140616594831920 -> 140616594830912
	140616594832016 -> 140616594830384
	140616683869408 [label="model.model.encoder.layer2.2.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	140616683869408 -> 140616594832016
	140616594832016 [label=AccumulateGrad]
	140616594831248 -> 140616594830384
	140616683869488 [label="model.model.encoder.layer2.2.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	140616683869488 -> 140616594831248
	140616594831248 [label=AccumulateGrad]
	140616594831056 -> 140616594832400
	140616683868768 [label="model.model.encoder.layer2.2.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	140616683868768 -> 140616594831056
	140616594831056 [label=AccumulateGrad]
	140616594831728 -> 140616594832400
	140616683869648 [label="model.model.encoder.layer2.2.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	140616683869648 -> 140616594831728
	140616594831728 [label=AccumulateGrad]
	140616594832352 -> 140616594828896
	140616594832544 -> 140616594833312
	140616683867888 [label="model.model.encoder.layer2.3.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140616683867888 -> 140616594832544
	140616594832544 [label=AccumulateGrad]
	140616594833360 -> 140616594833168
	140616683869808 [label="model.model.encoder.layer2.3.bn1.weight
 (256)" fillcolor=lightblue]
	140616683869808 -> 140616594833360
	140616594833360 [label=AccumulateGrad]
	140616594832832 -> 140616594833168
	140616683869888 [label="model.model.encoder.layer2.3.bn1.bias
 (256)" fillcolor=lightblue]
	140616683869888 -> 140616594832832
	140616594832832 [label=AccumulateGrad]
	140616594832880 -> 140616594829184
	140616683870608 [label="model.model.encoder.layer2.3.conv2.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	140616683870608 -> 140616594832880
	140616594832880 [label=AccumulateGrad]
	140616594828848 -> 140616594828368
	140616683870368 [label="model.model.encoder.layer2.3.bn2.weight
 (256)" fillcolor=lightblue]
	140616683870368 -> 140616594828848
	140616594828848 [label=AccumulateGrad]
	140616594828608 -> 140616594828368
	140616683870688 [label="model.model.encoder.layer2.3.bn2.bias
 (256)" fillcolor=lightblue]
	140616683870688 -> 140616594828608
	140616594828608 [label=AccumulateGrad]
	140616594822080 -> 140616594821792
	140616683871168 [label="model.model.encoder.layer2.3.conv3.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140616683871168 -> 140616594822080
	140616594822080 [label=AccumulateGrad]
	140616594821936 -> 140616594821648
	140616683871248 [label="model.model.encoder.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	140616683871248 -> 140616594821936
	140616594821936 [label=AccumulateGrad]
	140616594821504 -> 140616594821648
	140616683870768 [label="model.model.encoder.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	140616683870768 -> 140616594821504
	140616594821504 [label=AccumulateGrad]
	140616594821216 -> 140616594820928
	140616594821216 [label=SigmoidBackward0]
	140616594828704 -> 140616594821216
	140616594828704 [label=ConvolutionBackward0]
	140616594819392 -> 140616594828704
	140616594819392 [label=ReluBackward0]
	140616594832208 -> 140616594819392
	140616594832208 [label=ConvolutionBackward0]
	140616594829088 -> 140616594832208
	140616594829088 [label=MeanBackward1]
	140616594821648 -> 140616594829088
	140616594832496 -> 140616594832208
	140616683871008 [label="model.model.encoder.layer2.3.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	140616683871008 -> 140616594832496
	140616594832496 [label=AccumulateGrad]
	140616594833264 -> 140616594832208
	140616683871888 [label="model.model.encoder.layer2.3.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	140616683871888 -> 140616594833264
	140616594833264 [label=AccumulateGrad]
	140616594829280 -> 140616594828704
	140616683871488 [label="model.model.encoder.layer2.3.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	140616683871488 -> 140616594829280
	140616594829280 [label=AccumulateGrad]
	140616594822224 -> 140616594828704
	140616683872208 [label="model.model.encoder.layer2.3.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	140616683872208 -> 140616594822224
	140616594822224 [label=AccumulateGrad]
	140616594820976 -> 140616594821024
	140616594820640 -> 140616594817616
	140616683873168 [label="model.model.encoder.layer3.0.conv1.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140616683873168 -> 140616594820640
	140616594820640 [label=AccumulateGrad]
	140616594817952 -> 140616594817712
	140616683873248 [label="model.model.encoder.layer3.0.bn1.weight
 (512)" fillcolor=lightblue]
	140616683873248 -> 140616594817952
	140616594817952 [label=AccumulateGrad]
	140616594817424 -> 140616594817712
	140616683872528 [label="model.model.encoder.layer3.0.bn1.bias
 (512)" fillcolor=lightblue]
	140616683872528 -> 140616594817424
	140616594817424 [label=AccumulateGrad]
	140616594817376 -> 140616594819776
	140616683873328 [label="model.model.encoder.layer3.0.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616683873328 -> 140616594817376
	140616594817376 [label=AccumulateGrad]
	140616594818912 -> 140616594818576
	140616683873648 [label="model.model.encoder.layer3.0.bn2.weight
 (512)" fillcolor=lightblue]
	140616683873648 -> 140616594818912
	140616594818912 [label=AccumulateGrad]
	140616594820208 -> 140616594818576
	140616683873968 [label="model.model.encoder.layer3.0.bn2.bias
 (512)" fillcolor=lightblue]
	140616683873968 -> 140616594820208
	140616594820208 [label=AccumulateGrad]
	140616594820448 -> 140616594820112
	140616683873728 [label="model.model.encoder.layer3.0.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616683873728 -> 140616594820448
	140616594820448 [label=AccumulateGrad]
	140616594819824 -> 140616594819296
	140616683873568 [label="model.model.encoder.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	140616683873568 -> 140616594819824
	140616594819824 [label=AccumulateGrad]
	140616594819872 -> 140616594819296
	140616683873888 [label="model.model.encoder.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	140616683873888 -> 140616594819872
	140616594819872 [label=AccumulateGrad]
	140616594828944 -> 140616594695952
	140616594828944 [label=SigmoidBackward0]
	140616594817136 -> 140616594828944
	140616594817136 [label=ConvolutionBackward0]
	140616594818864 -> 140616594817136
	140616594818864 [label=ReluBackward0]
	140616594821360 -> 140616594818864
	140616594821360 [label=ConvolutionBackward0]
	140616594821072 -> 140616594821360
	140616594821072 [label=MeanBackward1]
	140616594819296 -> 140616594821072
	140616594818192 -> 140616594821360
	140616683874208 [label="model.model.encoder.layer3.0.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616683874208 -> 140616594818192
	140616594818192 [label=AccumulateGrad]
	140616594818240 -> 140616594821360
	140616683874368 [label="model.model.encoder.layer3.0.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616683874368 -> 140616594818240
	140616594818240 [label=AccumulateGrad]
	140616594820160 -> 140616594817136
	140616683875008 [label="model.model.encoder.layer3.0.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616683875008 -> 140616594820160
	140616594820160 [label=AccumulateGrad]
	140616594818624 -> 140616594817136
	140616683875088 [label="model.model.encoder.layer3.0.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616683875088 -> 140616594818624
	140616594818624 [label=AccumulateGrad]
	140616594695712 -> 140616594696528
	140616594695712 [label=CudnnBatchNormBackward0]
	140616594820784 -> 140616594695712
	140616594820784 [label=ConvolutionBackward0]
	140616594730496 -> 140616594820784
	140616594833216 -> 140616594820784
	140616683871568 [label="model.model.encoder.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616683871568 -> 140616594833216
	140616594833216 [label=AccumulateGrad]
	140616594817184 -> 140616594695712
	140616683872368 [label="model.model.encoder.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	140616683872368 -> 140616594817184
	140616594817184 [label=AccumulateGrad]
	140616594828320 -> 140616594695712
	140616683872448 [label="model.model.encoder.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	140616683872448 -> 140616594828320
	140616594828320 [label=AccumulateGrad]
	140616594697344 -> 140616594698784
	140616683874128 [label="model.model.encoder.layer3.1.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140616683874128 -> 140616594697344
	140616594697344 [label=AccumulateGrad]
	140616594698304 -> 140616594686112
	140616683876048 [label="model.model.encoder.layer3.1.bn1.weight
 (512)" fillcolor=lightblue]
	140616683876048 -> 140616594698304
	140616594698304 [label=AccumulateGrad]
	140616594686064 -> 140616594686112
	140616683874768 [label="model.model.encoder.layer3.1.bn1.bias
 (512)" fillcolor=lightblue]
	140616683874768 -> 140616594686064
	140616594686064 [label=AccumulateGrad]
	140616594688512 -> 140616594688992
	140616683878048 [label="model.model.encoder.layer3.1.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616683878048 -> 140616594688512
	140616594688512 [label=AccumulateGrad]
	140616594689520 -> 140616594689328
	140616683875168 [label="model.model.encoder.layer3.1.bn2.weight
 (512)" fillcolor=lightblue]
	140616683875168 -> 140616594689520
	140616594689520 [label=AccumulateGrad]
	140616594690144 -> 140616594689328
	140616683877648 [label="model.model.encoder.layer3.1.bn2.bias
 (512)" fillcolor=lightblue]
	140616683877648 -> 140616594690144
	140616594690144 [label=AccumulateGrad]
	140616594691872 -> 140616594693264
	140616683876288 [label="model.model.encoder.layer3.1.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616683876288 -> 140616594691872
	140616594691872 [label=AccumulateGrad]
	140616594693360 -> 140616594693888
	140616683877728 [label="model.model.encoder.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	140616683877728 -> 140616594693360
	140616594693360 [label=AccumulateGrad]
	140616594694272 -> 140616594693888
	140616683877968 [label="model.model.encoder.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	140616683877968 -> 140616594694272
	140616594694272 [label=AccumulateGrad]
	140616594700032 -> 140616594700608
	140616594700032 [label=SigmoidBackward0]
	140616594688128 -> 140616594700032
	140616594688128 [label=ConvolutionBackward0]
	140616594690384 -> 140616594688128
	140616594690384 [label=ReluBackward0]
	140616594698208 -> 140616594690384
	140616594698208 [label=ConvolutionBackward0]
	140616594696576 -> 140616594698208
	140616594696576 [label=MeanBackward1]
	140616594693888 -> 140616594696576
	140616594697728 -> 140616594698208
	140616683879408 [label="model.model.encoder.layer3.1.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616683879408 -> 140616594697728
	140616594697728 [label=AccumulateGrad]
	140616594817328 -> 140616594698208
	140616683879648 [label="model.model.encoder.layer3.1.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616683879648 -> 140616594817328
	140616594817328 [label=AccumulateGrad]
	140616594689088 -> 140616594688128
	140616683878608 [label="model.model.encoder.layer3.1.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616683878608 -> 140616594689088
	140616594689088 [label=AccumulateGrad]
	140616594692496 -> 140616594688128
	140616683878368 [label="model.model.encoder.layer3.1.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616683878368 -> 140616594692496
	140616594692496 [label=AccumulateGrad]
	140616594699984 -> 140616594701184
	140616594701040 -> 140616594687936
	140616683876848 [label="model.model.encoder.layer3.2.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140616683876848 -> 140616594701040
	140616594701040 [label=AccumulateGrad]
	140616594687648 -> 140616594690960
	140616683876928 [label="model.model.encoder.layer3.2.bn1.weight
 (512)" fillcolor=lightblue]
	140616683876928 -> 140616594687648
	140616594687648 [label=AccumulateGrad]
	140616594694416 -> 140616594690960
	140616683879728 [label="model.model.encoder.layer3.2.bn1.bias
 (512)" fillcolor=lightblue]
	140616683879728 -> 140616594694416
	140616594694416 [label=AccumulateGrad]
	140616594695136 -> 140616594695376
	140616683875248 [label="model.model.encoder.layer3.2.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616683875248 -> 140616594695136
	140616594695136 [label=AccumulateGrad]
	140616594693840 -> 140616594695904
	140616683875648 [label="model.model.encoder.layer3.2.bn2.weight
 (512)" fillcolor=lightblue]
	140616683875648 -> 140616594693840
	140616594693840 [label=AccumulateGrad]
	140616594696288 -> 140616594695904
	140616683878128 [label="model.model.encoder.layer3.2.bn2.bias
 (512)" fillcolor=lightblue]
	140616683878128 -> 140616594696288
	140616594696288 [label=AccumulateGrad]
	140616594696480 -> 140616594697152
	140616694689248 [label="model.model.encoder.layer3.2.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616694689248 -> 140616594696480
	140616594696480 [label=AccumulateGrad]
	140616594697104 -> 140616594696720
	140616694689408 [label="model.model.encoder.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	140616694689408 -> 140616594697104
	140616594697104 [label=AccumulateGrad]
	140616594697392 -> 140616594696720
	140616694689008 [label="model.model.encoder.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	140616694689008 -> 140616594697392
	140616594697392 [label=AccumulateGrad]
	140616594697296 -> 140616594697968
	140616594697296 [label=SigmoidBackward0]
	140616594694752 -> 140616594697296
	140616594694752 [label=ConvolutionBackward0]
	140616594696192 -> 140616594694752
	140616594696192 [label=ReluBackward0]
	140616594699408 -> 140616594696192
	140616594699408 [label=ConvolutionBackward0]
	140616594700560 -> 140616594699408
	140616594700560 [label=MeanBackward1]
	140616594696720 -> 140616594700560
	140616594687360 -> 140616594699408
	140616694688608 [label="model.model.encoder.layer3.2.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616694688608 -> 140616594687360
	140616594687360 [label=AccumulateGrad]
	140616594686976 -> 140616594699408
	140616694688688 [label="model.model.encoder.layer3.2.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616694688688 -> 140616594686976
	140616594686976 [label=AccumulateGrad]
	140616594701904 -> 140616594694752
	140616694687968 [label="model.model.encoder.layer3.2.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616694687968 -> 140616594701904
	140616594701904 [label=AccumulateGrad]
	140616594696864 -> 140616594694752
	140616694688288 [label="model.model.encoder.layer3.2.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616694688288 -> 140616594696864
	140616594696864 [label=AccumulateGrad]
	140616594697872 -> 140616594698160
	140616594698544 -> 140616594699120
	140616694688768 [label="model.model.encoder.layer3.3.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140616694688768 -> 140616594698544
	140616594698544 [label=AccumulateGrad]
	140616594699024 -> 140616594686400
	140616694689328 [label="model.model.encoder.layer3.3.bn1.weight
 (512)" fillcolor=lightblue]
	140616694689328 -> 140616594699024
	140616594699024 [label=AccumulateGrad]
	140616594686784 -> 140616594686400
	140616694688368 [label="model.model.encoder.layer3.3.bn1.bias
 (512)" fillcolor=lightblue]
	140616694688368 -> 140616594686784
	140616594686784 [label=AccumulateGrad]
	140616594688176 -> 140616594688704
	140616694677808 [label="model.model.encoder.layer3.3.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616694677808 -> 140616594688176
	140616594688176 [label=AccumulateGrad]
	140616594688752 -> 140616594688656
	140616694677728 [label="model.model.encoder.layer3.3.bn2.weight
 (512)" fillcolor=lightblue]
	140616694677728 -> 140616594688752
	140616594688752 [label=AccumulateGrad]
	140616594689280 -> 140616594688656
	140616694677888 [label="model.model.encoder.layer3.3.bn2.bias
 (512)" fillcolor=lightblue]
	140616694677888 -> 140616594689280
	140616594689280 [label=AccumulateGrad]
	140616594689664 -> 140616594690192
	140616694678368 [label="model.model.encoder.layer3.3.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616694678368 -> 140616594689664
	140616594689664 [label=AccumulateGrad]
	140616594689808 -> 140616594690480
	140616694678448 [label="model.model.encoder.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	140616694678448 -> 140616594689808
	140616594689808 [label=AccumulateGrad]
	140616594690528 -> 140616594690480
	140616694678528 [label="model.model.encoder.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	140616694678528 -> 140616594690528
	140616594690528 [label=AccumulateGrad]
	140616594691920 -> 140616594691824
	140616594691920 [label=SigmoidBackward0]
	140616594686352 -> 140616594691920
	140616594686352 [label=ConvolutionBackward0]
	140616594688368 -> 140616594686352
	140616594688368 [label=ReluBackward0]
	140616594697920 -> 140616594688368
	140616594697920 [label=ConvolutionBackward0]
	140616594698496 -> 140616594697920
	140616594698496 [label=MeanBackward1]
	140616594690480 -> 140616594698496
	140616594698736 -> 140616594697920
	140616694679008 [label="model.model.encoder.layer3.3.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616694679008 -> 140616594698736
	140616594698736 [label=AccumulateGrad]
	140616594699072 -> 140616594697920
	140616694679088 [label="model.model.encoder.layer3.3.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616694679088 -> 140616594699072
	140616594699072 [label=AccumulateGrad]
	140616594688080 -> 140616594686352
	140616694679248 [label="model.model.encoder.layer3.3.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616694679248 -> 140616594688080
	140616594688080 [label=AccumulateGrad]
	140616594689904 -> 140616594686352
	140616694679328 [label="model.model.encoder.layer3.3.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616694679328 -> 140616594689904
	140616594689904 [label=AccumulateGrad]
	140616594692448 -> 140616594692736
	140616594693120 -> 140616594693696
	140616694679488 [label="model.model.encoder.layer3.4.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140616694679488 -> 140616594693120
	140616594693120 [label=AccumulateGrad]
	140616594693648 -> 140616594693936
	140616694679568 [label="model.model.encoder.layer3.4.bn1.weight
 (512)" fillcolor=lightblue]
	140616694679568 -> 140616594693648
	140616594693648 [label=AccumulateGrad]
	140616594694128 -> 140616594693936
	140616694679648 [label="model.model.encoder.layer3.4.bn1.bias
 (512)" fillcolor=lightblue]
	140616694679648 -> 140616594694128
	140616594694128 [label=AccumulateGrad]
	140616594699648 -> 140616594699936
	140616694680128 [label="model.model.encoder.layer3.4.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616694680128 -> 140616594699648
	140616594699648 [label=AccumulateGrad]
	140616594700224 -> 140616594700320
	140616694680048 [label="model.model.encoder.layer3.4.bn2.weight
 (512)" fillcolor=lightblue]
	140616694680048 -> 140616594700224
	140616594700224 [label=AccumulateGrad]
	140616594700512 -> 140616594700320
	140616694680208 [label="model.model.encoder.layer3.4.bn2.bias
 (512)" fillcolor=lightblue]
	140616694680208 -> 140616594700512
	140616594700512 [label=AccumulateGrad]
	140616594700896 -> 140616594701376
	140616694680608 [label="model.model.encoder.layer3.4.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616694680608 -> 140616594700896
	140616594700896 [label=AccumulateGrad]
	140616594701472 -> 140616594701712
	140616694680688 [label="model.model.encoder.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	140616694680688 -> 140616594701472
	140616594701472 [label=AccumulateGrad]
	140616594701760 -> 140616594701712
	140616694680768 [label="model.model.encoder.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	140616694680768 -> 140616594701760
	140616594701760 [label=AccumulateGrad]
	140616594687024 -> 140616594687216
	140616594687024 [label=SigmoidBackward0]
	140616594699744 -> 140616594687024
	140616594699744 [label=ConvolutionBackward0]
	140616594699600 -> 140616594699744
	140616594699600 [label=ReluBackward0]
	140616594690096 -> 140616594699600
	140616594690096 [label=ConvolutionBackward0]
	140616594692832 -> 140616594690096
	140616594692832 [label=MeanBackward1]
	140616594701712 -> 140616594692832
	140616594693312 -> 140616594690096
	140616694681168 [label="model.model.encoder.layer3.4.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616694681168 -> 140616594693312
	140616594693312 [label=AccumulateGrad]
	140616594693408 -> 140616594690096
	140616694681248 [label="model.model.encoder.layer3.4.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616694681248 -> 140616594693408
	140616594693408 [label=AccumulateGrad]
	140616594697584 -> 140616594699744
	140616694681408 [label="model.model.encoder.layer3.4.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616694681408 -> 140616594697584
	140616594697584 [label=AccumulateGrad]
	140616594701088 -> 140616594699744
	140616694681488 [label="model.model.encoder.layer3.4.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616694681488 -> 140616594701088
	140616594701088 [label=AccumulateGrad]
	140616594687552 -> 140616594687600
	140616594687792 -> 140616594691392
	140616694681648 [label="model.model.encoder.layer3.5.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140616694681648 -> 140616594687792
	140616594687792 [label=AccumulateGrad]
	140616594691344 -> 140616594691632
	140616694681728 [label="model.model.encoder.layer3.5.bn1.weight
 (512)" fillcolor=lightblue]
	140616694681728 -> 140616594691344
	140616594691344 [label=AccumulateGrad]
	140616594694560 -> 140616594691632
	140616694681808 [label="model.model.encoder.layer3.5.bn1.bias
 (512)" fillcolor=lightblue]
	140616694681808 -> 140616594694560
	140616594694560 [label=AccumulateGrad]
	140616594694800 -> 140616594695328
	140616694682368 [label="model.model.encoder.layer3.5.conv2.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	140616694682368 -> 140616594694800
	140616594694800 [label=AccumulateGrad]
	140616594701952 -> 140616594702048
	140616694682128 [label="model.model.encoder.layer3.5.bn2.weight
 (512)" fillcolor=lightblue]
	140616694682128 -> 140616594701952
	140616594701952 [label=AccumulateGrad]
	140616594702192 -> 140616594702048
	140616694682288 [label="model.model.encoder.layer3.5.bn2.bias
 (512)" fillcolor=lightblue]
	140616694682288 -> 140616594702192
	140616594702192 [label=AccumulateGrad]
	140616594724208 -> 140616594724976
	140616694682848 [label="model.model.encoder.layer3.5.conv3.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140616694682848 -> 140616594724208
	140616594724208 [label=AccumulateGrad]
	140616594725360 -> 140616594725888
	140616694682928 [label="model.model.encoder.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	140616694682928 -> 140616594725360
	140616594725360 [label=AccumulateGrad]
	140616594726272 -> 140616594725888
	140616694683008 [label="model.model.encoder.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	140616694683008 -> 140616594726272
	140616594726272 [label=AccumulateGrad]
	140616594727136 -> 140616594727280
	140616594727136 [label=SigmoidBackward0]
	140616594724688 -> 140616594727136
	140616594724688 [label=ConvolutionBackward0]
	140616594702288 -> 140616594724688
	140616594702288 [label=ReluBackward0]
	140616594686928 -> 140616594702288
	140616594686928 [label=ConvolutionBackward0]
	140616594687504 -> 140616594686928
	140616594687504 [label=MeanBackward1]
	140616594725888 -> 140616594687504
	140616594691008 -> 140616594686928
	140616694683488 [label="model.model.encoder.layer3.5.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	140616694683488 -> 140616594691008
	140616594691008 [label=AccumulateGrad]
	140616594691104 -> 140616594686928
	140616694683568 [label="model.model.encoder.layer3.5.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	140616694683568 -> 140616594691104
	140616594691104 [label=AccumulateGrad]
	140616594694992 -> 140616594724688
	140616694683728 [label="model.model.encoder.layer3.5.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	140616694683728 -> 140616594694992
	140616594694992 [label=AccumulateGrad]
	140616594694704 -> 140616594724688
	140616694683808 [label="model.model.encoder.layer3.5.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	140616694683808 -> 140616594694704
	140616594694704 [label=AccumulateGrad]
	140616594727088 -> 140616594733472
	140616594734048 -> 140616594720800
	140616694684448 [label="model.model.encoder.layer4.0.conv1.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	140616694684448 -> 140616594734048
	140616594734048 [label=AccumulateGrad]
	140616594734192 -> 140616594721376
	140616694684528 [label="model.model.encoder.layer4.0.bn1.weight
 (1024)" fillcolor=lightblue]
	140616694684528 -> 140616594734192
	140616594734192 [label=AccumulateGrad]
	140616594721952 -> 140616594721376
	140616694684608 [label="model.model.encoder.layer4.0.bn1.bias
 (1024)" fillcolor=lightblue]
	140616694684608 -> 140616594721952
	140616594721952 [label=AccumulateGrad]
	140616594721520 -> 140616594722720
	140616694685168 [label="model.model.encoder.layer4.0.conv2.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	140616694685168 -> 140616594721520
	140616594721520 [label=AccumulateGrad]
	140616594719552 -> 140616594719216
	140616694685088 [label="model.model.encoder.layer4.0.bn2.weight
 (1024)" fillcolor=lightblue]
	140616694685088 -> 140616594719552
	140616594719552 [label=AccumulateGrad]
	140616594719888 -> 140616594719216
	140616694685248 [label="model.model.encoder.layer4.0.bn2.bias
 (1024)" fillcolor=lightblue]
	140616694685248 -> 140616594719888
	140616594719888 [label=AccumulateGrad]
	140616594723296 -> 140616594728432
	140616694685728 [label="model.model.encoder.layer4.0.conv3.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140616694685728 -> 140616594723296
	140616594723296 [label=AccumulateGrad]
	140616594728240 -> 140616594729104
	140616694685808 [label="model.model.encoder.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	140616694685808 -> 140616594728240
	140616594728240 [label=AccumulateGrad]
	140616594728720 -> 140616594729104
	140616694685888 [label="model.model.encoder.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	140616694685888 -> 140616594728720
	140616594728720 [label=AccumulateGrad]
	140616594730016 -> 140616594730880
	140616594730016 [label=SigmoidBackward0]
	140616594721904 -> 140616594730016
	140616594721904 [label=ConvolutionBackward0]
	140616594720080 -> 140616594721904
	140616594720080 [label=ReluBackward0]
	140616594726512 -> 140616594720080
	140616594726512 [label=ConvolutionBackward0]
	140616594733088 -> 140616594726512
	140616594733088 [label=MeanBackward1]
	140616594729104 -> 140616594733088
	140616594734480 -> 140616594726512
	140616694686288 [label="model.model.encoder.layer4.0.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	140616694686288 -> 140616594734480
	140616594734480 [label=AccumulateGrad]
	140616594734288 -> 140616594726512
	140616694686368 [label="model.model.encoder.layer4.0.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	140616694686368 -> 140616594734288
	140616594734288 [label=AccumulateGrad]
	140616594718976 -> 140616594721904
	140616694686528 [label="model.model.encoder.layer4.0.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	140616694686528 -> 140616594718976
	140616594718976 [label=AccumulateGrad]
	140616594723872 -> 140616594721904
	140616694686608 [label="model.model.encoder.layer4.0.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	140616694686608 -> 140616594723872
	140616594723872 [label=AccumulateGrad]
	140616594730256 -> 140616594731456
	140616594730256 [label=CudnnBatchNormBackward0]
	140616594733952 -> 140616594730256
	140616594733952 [label=ConvolutionBackward0]
	140616594723248 -> 140616594733952
	140616594724784 -> 140616594733952
	140616694683168 [label="model.model.encoder.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140616694683168 -> 140616594724784
	140616594724784 [label=AccumulateGrad]
	140616594728000 -> 140616594730256
	140616694683888 [label="model.model.encoder.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	140616694683888 -> 140616594728000
	140616594728000 [label=AccumulateGrad]
	140616594729632 -> 140616594730256
	140616694683968 [label="model.model.encoder.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	140616694683968 -> 140616594729632
	140616594729632 [label=AccumulateGrad]
	140616594732032 -> 140616594724256
	140616694686768 [label="model.model.encoder.layer4.1.conv1.weight
 (1024, 2048, 1, 1)" fillcolor=lightblue]
	140616694686768 -> 140616594732032
	140616594732032 [label=AccumulateGrad]
	140616594732800 -> 140616594724496
	140616694686848 [label="model.model.encoder.layer4.1.bn1.weight
 (1024)" fillcolor=lightblue]
	140616694686848 -> 140616594732800
	140616594732800 [label=AccumulateGrad]
	140616594724832 -> 140616594724496
	140616694686928 [label="model.model.encoder.layer4.1.bn1.bias
 (1024)" fillcolor=lightblue]
	140616694686928 -> 140616594724832
	140616594724832 [label=AccumulateGrad]
	140616594725072 -> 140616594725120
	140616694687488 [label="model.model.encoder.layer4.1.conv2.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	140616694687488 -> 140616594725072
	140616594725072 [label=AccumulateGrad]
	140616594725648 -> 140616594725312
	140616694687408 [label="model.model.encoder.layer4.1.bn2.weight
 (1024)" fillcolor=lightblue]
	140616694687408 -> 140616594725648
	140616594725648 [label=AccumulateGrad]
	140616594725696 -> 140616594725312
	140616694687568 [label="model.model.encoder.layer4.1.bn2.bias
 (1024)" fillcolor=lightblue]
	140616694687568 -> 140616594725696
	140616594725696 [label=AccumulateGrad]
	140616594725936 -> 140616594727040
	140616712594080 [label="model.model.encoder.layer4.1.conv3.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140616712594080 -> 140616594725936
	140616594725936 [label=AccumulateGrad]
	140616594726752 -> 140616594726704
	140616712594160 [label="model.model.encoder.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	140616712594160 -> 140616594726752
	140616594726752 [label=AccumulateGrad]
	140616594727424 -> 140616594726704
	140616712594240 [label="model.model.encoder.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	140616712594240 -> 140616594727424
	140616594727424 [label=AccumulateGrad]
	140616594727664 -> 140616594733040
	140616594727664 [label=SigmoidBackward0]
	140616594724736 -> 140616594727664
	140616594724736 [label=ConvolutionBackward0]
	140616594725264 -> 140616594724736
	140616594725264 [label=ReluBackward0]
	140616594720752 -> 140616594725264
	140616594720752 [label=ConvolutionBackward0]
	140616594730832 -> 140616594720752
	140616594730832 [label=MeanBackward1]
	140616594726704 -> 140616594730832
	140616594731600 -> 140616594720752
	140616712594720 [label="model.model.encoder.layer4.1.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	140616712594720 -> 140616594731600
	140616594731600 [label=AccumulateGrad]
	140616594731984 -> 140616594720752
	140616712594800 [label="model.model.encoder.layer4.1.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	140616712594800 -> 140616594731984
	140616594731984 [label=AccumulateGrad]
	140616594725408 -> 140616594724736
	140616712594960 [label="model.model.encoder.layer4.1.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	140616712594960 -> 140616594725408
	140616594725408 [label=AccumulateGrad]
	140616594725840 -> 140616594724736
	140616712595040 [label="model.model.encoder.layer4.1.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	140616712595040 -> 140616594725840
	140616594725840 [label=AccumulateGrad]
	140616594727712 -> 140616594733664
	140616594733616 -> 140616594734624
	140616712595200 [label="model.model.encoder.layer4.2.conv1.weight
 (1024, 2048, 1, 1)" fillcolor=lightblue]
	140616712595200 -> 140616594733616
	140616594733616 [label=AccumulateGrad]
	140616594733328 -> 140616594734864
	140616712595280 [label="model.model.encoder.layer4.2.bn1.weight
 (1024)" fillcolor=lightblue]
	140616712595280 -> 140616594733328
	140616594733328 [label=AccumulateGrad]
	140616594720512 -> 140616594734864
	140616712595360 [label="model.model.encoder.layer4.2.bn1.bias
 (1024)" fillcolor=lightblue]
	140616712595360 -> 140616594720512
	140616594720512 [label=AccumulateGrad]
	140616594720704 -> 140616594720368
	140616712595920 [label="model.model.encoder.layer4.2.conv2.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	140616712595920 -> 140616594720704
	140616594720704 [label=AccumulateGrad]
	140616594721280 -> 140616594720992
	140616712595840 [label="model.model.encoder.layer4.2.bn2.weight
 (1024)" fillcolor=lightblue]
	140616712595840 -> 140616594721280
	140616594721280 [label=AccumulateGrad]
	140616594720944 -> 140616594720992
	140616712596000 [label="model.model.encoder.layer4.2.bn2.bias
 (1024)" fillcolor=lightblue]
	140616712596000 -> 140616594720944
	140616594720944 [label=AccumulateGrad]
	140616594721568 -> 140616594722480
	140616712596400 [label="model.model.encoder.layer4.2.conv3.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140616712596400 -> 140616594721568
	140616594721568 [label=AccumulateGrad]
	140616594722144 -> 140616594722528
	140616712596480 [label="model.model.encoder.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	140616712596480 -> 140616594722144
	140616594722144 [label=AccumulateGrad]
	140616594722672 -> 140616594722528
	140616712596560 [label="model.model.encoder.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	140616712596560 -> 140616594722672
	140616594722672 [label=AccumulateGrad]
	140616594718928 -> 140616594719312
	140616594718928 [label=SigmoidBackward0]
	140616594720416 -> 140616594718928
	140616594720416 [label=ConvolutionBackward0]
	140616594721664 -> 140616594720416
	140616594721664 [label=ReluBackward0]
	140616594727328 -> 140616594721664
	140616594727328 [label=ConvolutionBackward0]
	140616594733136 -> 140616594727328
	140616594733136 [label=MeanBackward1]
	140616594722528 -> 140616594733136
	140616594734240 -> 140616594727328
	140616712597040 [label="model.model.encoder.layer4.2.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	140616712597040 -> 140616594734240
	140616594734240 [label=AccumulateGrad]
	140616594733712 -> 140616594727328
	140616712597120 [label="model.model.encoder.layer4.2.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	140616712597120 -> 140616594733712
	140616594733712 [label=AccumulateGrad]
	140616594721088 -> 140616594720416
	140616712597280 [label="model.model.encoder.layer4.2.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	140616712597280 -> 140616594721088
	140616594721088 [label=AccumulateGrad]
	140616594721232 -> 140616594720416
	140616712597360 [label="model.model.encoder.layer4.2.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	140616712597360 -> 140616594721232
	140616594721232 [label=AccumulateGrad]
	140616594719024 -> 140616594719840
	140616594723248 -> 140616594723632
	140616594723344 -> 140616594728192
	140616594247488 [label="model.model.decoder.blocks.0.conv1.0.weight
 (256, 3072, 3, 3)" fillcolor=lightblue]
	140616594247488 -> 140616594723344
	140616594723344 [label=AccumulateGrad]
	140616594723920 -> 140616594728576
	140616594244448 [label="model.model.decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	140616594244448 -> 140616594723920
	140616594723920 [label=AccumulateGrad]
	140616594728816 -> 140616594728576
	140616594256688 [label="model.model.decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	140616594256688 -> 140616594728816
	140616594728816 [label=AccumulateGrad]
	140616594729152 -> 140616594729056
	140616683799232 [label="model.model.decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140616683799232 -> 140616594729152
	140616594729152 [label=AccumulateGrad]
	140616594729584 -> 140616594729440
	140616683811792 [label="model.model.decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	140616683811792 -> 140616594729584
	140616594729584 [label=AccumulateGrad]
	140616594729008 -> 140616594729440
	140616683810112 [label="model.model.decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	140616683810112 -> 140616594729008
	140616594729008 [label=AccumulateGrad]
	140616594730496 -> 140616594729872
	140616594731360 -> 140616594731024
	140616683812272 [label="model.model.decoder.blocks.1.conv1.0.weight
 (128, 768, 3, 3)" fillcolor=lightblue]
	140616683812272 -> 140616594731360
	140616594731360 [label=AccumulateGrad]
	140616594731936 -> 140616594731648
	140616683802192 [label="model.model.decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	140616683802192 -> 140616594731936
	140616594731936 [label=AccumulateGrad]
	140616594730736 -> 140616594731648
	140616683812592 [label="model.model.decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	140616683812592 -> 140616594730736
	140616594730736 [label=AccumulateGrad]
	140616594732224 -> 140617065896288
	140616683814032 [label="model.model.decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140616683814032 -> 140616594732224
	140616594732224 [label=AccumulateGrad]
	140616608345600 -> 140616608345792
	140616683814832 [label="model.model.decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	140616683814832 -> 140616608345600
	140616608345600 [label=AccumulateGrad]
	140616594732848 -> 140616608345792
	140616683807872 [label="model.model.decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	140616683807872 -> 140616594732848
	140616594732848 [label=AccumulateGrad]
	140616594176464 -> 140616594166384
	140616594175936 -> 140616594175600
	140616683814672 [label="model.model.decoder.blocks.2.conv1.0.weight
 (64, 384, 3, 3)" fillcolor=lightblue]
	140616683814672 -> 140616594175936
	140616594175936 [label=AccumulateGrad]
	140616594175696 -> 140616594175408
	140616683811392 [label="model.model.decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	140616683811392 -> 140616594175696
	140616594175696 [label=AccumulateGrad]
	140616594175744 -> 140616594175408
	140616683809952 [label="model.model.decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	140616683809952 -> 140616594175744
	140616594175744 [label=AccumulateGrad]
	140616594175984 -> 140616594176848
	140616683813312 [label="model.model.decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140616683813312 -> 140616594175984
	140616594175984 [label=AccumulateGrad]
	140616594176560 -> 140616594177040
	140616683812832 [label="model.model.decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	140616683812832 -> 140616594176560
	140616594176560 [label=AccumulateGrad]
	140616594164944 -> 140616594177040
	140616683813712 [label="model.model.decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	140616683813712 -> 140616594164944
	140616594164944 [label=AccumulateGrad]
	140616594175360 -> 140616594177904
	140616594177616 -> 140616594177712
	140616683809632 [label="model.model.decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	140616683809632 -> 140616594177616
	140616594177616 [label=AccumulateGrad]
	140616594176752 -> 140616594177664
	140616683802352 [label="model.model.decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	140616683802352 -> 140616594176752
	140616594176752 [label=AccumulateGrad]
	140616594177424 -> 140616594177664
	140616683799072 [label="model.model.decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	140616683799072 -> 140616594177424
	140616594177424 [label=AccumulateGrad]
	140616594177376 -> 140616594175024
	140616683805952 [label="model.model.decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	140616683805952 -> 140616594177376
	140616594177376 [label=AccumulateGrad]
	140616594175072 -> 140616594175168
	140616683802752 [label="model.model.decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	140616683802752 -> 140616594175072
	140616594175072 [label=AccumulateGrad]
	140616594174880 -> 140616594175168
	140616683804032 [label="model.model.decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	140616683804032 -> 140616594174880
	140616594174880 [label=AccumulateGrad]
	140616594173872 -> 140616594174256
	140616683806272 [label="model.model.decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	140616683806272 -> 140616594173872
	140616594173872 [label=AccumulateGrad]
	140616594174160 -> 140616594174208
	140616683805552 [label="model.model.decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	140616683805552 -> 140616594174160
	140616594174160 [label=AccumulateGrad]
	140616594173968 -> 140616594174208
	140616683806672 [label="model.model.decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	140616683806672 -> 140616594173968
	140616594173968 [label=AccumulateGrad]
	140616594173920 -> 140616594173008
	140616683804592 [label="model.model.decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	140616683804592 -> 140616594173920
	140616594173920 [label=AccumulateGrad]
	140616594173632 -> 140616594173104
	140616683806912 [label="model.model.decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	140616683806912 -> 140616594173632
	140616594173632 [label=AccumulateGrad]
	140616594173440 -> 140616594173104
	140616683803632 [label="model.model.decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	140616683803632 -> 140616594173440
	140616594173440 [label=AccumulateGrad]
	140616594173344 -> 140616594173056
	140616683809232 [label="model.model.segmentation_head.0.weight
 (1, 16, 3, 3)" fillcolor=lightblue]
	140616683809232 -> 140616594173344
	140616594173344 [label=AccumulateGrad]
	140616594173296 -> 140616594173056
	140616683804112 [label="model.model.segmentation_head.0.bias
 (1)" fillcolor=lightblue]
	140616683804112 -> 140616594173296
	140616594173296 [label=AccumulateGrad]
	140616594173056 -> 140616712599360
}
