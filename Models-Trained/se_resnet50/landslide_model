digraph {
	graph [size="250.95,250.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139944476290848 [label="
 (1, 1, 128, 128)" fillcolor=darkolivegreen1]
	139944475531616 [label=ConvolutionBackward0]
	139944475534208 -> 139944475531616
	139944475534208 [label=ReluBackward0]
	139944475532480 -> 139944475534208
	139944475532480 [label=CudnnBatchNormBackward0]
	139944475536368 -> 139944475532480
	139944475536368 [label=ConvolutionBackward0]
	139944475538096 -> 139944475536368
	139944475538096 [label=ReluBackward0]
	139944475538528 -> 139944475538096
	139944475538528 [label=CudnnBatchNormBackward0]
	139944475539392 -> 139944475538528
	139944475539392 [label=ConvolutionBackward0]
	139944475541120 -> 139944475539392
	139944475541120 [label=UpsampleNearest2DBackward0]
	139944475534832 -> 139944475541120
	139944475534832 [label=ReluBackward0]
	139944475533008 -> 139944475534832
	139944475533008 [label=CudnnBatchNormBackward0]
	139944475528352 -> 139944475533008
	139944475528352 [label=ConvolutionBackward0]
	139944475537136 -> 139944475528352
	139944475537136 [label=ReluBackward0]
	139944475539152 -> 139944475537136
	139944475539152 [label=CudnnBatchNormBackward0]
	139944475528256 -> 139944475539152
	139944475528256 [label=ConvolutionBackward0]
	139944475538864 -> 139944475528256
	139944475538864 [label=CatBackward0]
	139944475529840 -> 139944475538864
	139944475529840 [label=UpsampleNearest2DBackward0]
	139944475532384 -> 139944475529840
	139944475532384 [label=ReluBackward0]
	139944475532960 -> 139944475532384
	139944475532960 [label=CudnnBatchNormBackward0]
	139944475534016 -> 139944475532960
	139944475534016 [label=ConvolutionBackward0]
	139944475723328 -> 139944475534016
	139944475723328 [label=ReluBackward0]
	139944475725488 -> 139944475723328
	139944475725488 [label=CudnnBatchNormBackward0]
	139944475726352 -> 139944475725488
	139944475726352 [label=ConvolutionBackward0]
	139944475728080 -> 139944475726352
	139944475728080 [label=CatBackward0]
	139944475728512 -> 139944475728080
	139944475728512 [label=UpsampleNearest2DBackward0]
	139944475730672 -> 139944475728512
	139944475730672 [label=ReluBackward0]
	139944475731536 -> 139944475730672
	139944475731536 [label=CudnnBatchNormBackward0]
	139944475732400 -> 139944475731536
	139944475732400 [label=ConvolutionBackward0]
	139944475734128 -> 139944475732400
	139944475734128 [label=ReluBackward0]
	139944475734560 -> 139944475734128
	139944475734560 [label=CudnnBatchNormBackward0]
	139944475735424 -> 139944475734560
	139944475735424 [label=ConvolutionBackward0]
	139944475737152 -> 139944475735424
	139944475737152 [label=CatBackward0]
	139944475735616 -> 139944475737152
	139944475735616 [label=UpsampleNearest2DBackward0]
	139944475731872 -> 139944475735616
	139944475731872 [label=ReluBackward0]
	139944475727456 -> 139944475731872
	139944475727456 [label=CudnnBatchNormBackward0]
	139944475725824 -> 139944475727456
	139944475725824 [label=ConvolutionBackward0]
	139943686110560 -> 139944475725824
	139943686110560 [label=ReluBackward0]
	139943686109216 -> 139943686110560
	139943686109216 [label=CudnnBatchNormBackward0]
	139944139989168 -> 139943686109216
	139944139989168 [label=ConvolutionBackward0]
	139944139995456 -> 139944139989168
	139944139995456 [label=CatBackward0]
	139944139991856 -> 139944139995456
	139944139991856 [label=UpsampleNearest2DBackward0]
	139944139996368 -> 139944139991856
	139944139996368 [label=ReluBackward0]
	139944139989072 -> 139944139996368
	139944139989072 [label=AddBackward0]
	139944139992720 -> 139944139989072
	139944139992720 [label=MulBackward0]
	139944139994928 -> 139944139992720
	139944139994928 [label=CudnnBatchNormBackward0]
	139944139996272 -> 139944139994928
	139944139996272 [label=ConvolutionBackward0]
	139944139990656 -> 139944139996272
	139944139990656 [label=ReluBackward0]
	139944139993488 -> 139944139990656
	139944139993488 [label=CudnnBatchNormBackward0]
	139944139992912 -> 139944139993488
	139944139992912 [label=ConvolutionBackward0]
	139944139987680 -> 139944139992912
	139944139987680 [label=ReluBackward0]
	139944139987008 -> 139944139987680
	139944139987008 [label=CudnnBatchNormBackward0]
	139944140799664 -> 139944139987008
	139944140799664 [label=ConvolutionBackward0]
	139944139994496 -> 139944140799664
	139944139994496 [label=ReluBackward0]
	139944140786224 -> 139944139994496
	139944140786224 [label=AddBackward0]
	139944140795344 -> 139944140786224
	139944140795344 [label=MulBackward0]
	139944140786560 -> 139944140795344
	139944140786560 [label=CudnnBatchNormBackward0]
	139944140799808 -> 139944140786560
	139944140799808 [label=ConvolutionBackward0]
	139944140798224 -> 139944140799808
	139944140798224 [label=ReluBackward0]
	139944140798272 -> 139944140798224
	139944140798272 [label=CudnnBatchNormBackward0]
	139944140798368 -> 139944140798272
	139944140798368 [label=ConvolutionBackward0]
	139944140798560 -> 139944140798368
	139944140798560 [label=ReluBackward0]
	139944140798992 -> 139944140798560
	139944140798992 [label=CudnnBatchNormBackward0]
	139944140799136 -> 139944140798992
	139944140799136 [label=ConvolutionBackward0]
	139944140790928 -> 139944140799136
	139944140790928 [label=ReluBackward0]
	139944140799760 -> 139944140790928
	139944140799760 [label=AddBackward0]
	139944140799904 -> 139944140799760
	139944140799904 [label=MulBackward0]
	139944140799952 -> 139944140799904
	139944140799952 [label=CudnnBatchNormBackward0]
	139944140793712 -> 139944140799952
	139944140793712 [label=ConvolutionBackward0]
	139944140791984 -> 139944140793712
	139944140791984 [label=ReluBackward0]
	139944140796208 -> 139944140791984
	139944140796208 [label=CudnnBatchNormBackward0]
	139944140795440 -> 139944140796208
	139944140795440 [label=ConvolutionBackward0]
	139944140799088 -> 139944140795440
	139944140799088 [label=ReluBackward0]
	139944140786320 -> 139944140799088
	139944140786320 [label=CudnnBatchNormBackward0]
	139944140786176 -> 139944140786320
	139944140786176 [label=ConvolutionBackward0]
	139944139990944 -> 139944140786176
	139944139990944 [label=ReluBackward0]
	139944140784256 -> 139944139990944
	139944140784256 [label=AddBackward0]
	139944140784112 -> 139944140784256
	139944140784112 [label=MulBackward0]
	139944140785600 -> 139944140784112
	139944140785600 [label=CudnnBatchNormBackward0]
	139944140783680 -> 139944140785600
	139944140783680 [label=ConvolutionBackward0]
	139944140784688 -> 139944140783680
	139944140784688 [label=ReluBackward0]
	139944140784592 -> 139944140784688
	139944140784592 [label=CudnnBatchNormBackward0]
	139944140784448 -> 139944140784592
	139944140784448 [label=ConvolutionBackward0]
	139944140784160 -> 139944140784448
	139944140784160 [label=ReluBackward0]
	139944140784064 -> 139944140784160
	139944140784064 [label=CudnnBatchNormBackward0]
	139944140783920 -> 139944140784064
	139944140783920 [label=ConvolutionBackward0]
	139944140785888 -> 139944140783920
	139944140785888 [label=ReluBackward0]
	139944177608320 -> 139944140785888
	139944177608320 [label=AddBackward0]
	139944177610624 -> 139944177608320
	139944177610624 [label=MulBackward0]
	139944177608224 -> 139944177610624
	139944177608224 [label=CudnnBatchNormBackward0]
	139944177607216 -> 139944177608224
	139944177607216 [label=ConvolutionBackward0]
	139944140927088 -> 139944177607216
	139944140927088 [label=ReluBackward0]
	139944140925456 -> 139944140927088
	139944140925456 [label=CudnnBatchNormBackward0]
	139944140922816 -> 139944140925456
	139944140922816 [label=ConvolutionBackward0]
	139944140918016 -> 139944140922816
	139944140918016 [label=ReluBackward0]
	139944140926800 -> 139944140918016
	139944140926800 [label=CudnnBatchNormBackward0]
	139944140914752 -> 139944140926800
	139944140914752 [label=ConvolutionBackward0]
	139944177606784 -> 139944140914752
	139944177606784 [label=ReluBackward0]
	139944140918976 -> 139944177606784
	139944140918976 [label=AddBackward0]
	139944140918688 -> 139944140918976
	139944140918688 [label=MulBackward0]
	139944140923824 -> 139944140918688
	139944140923824 [label=CudnnBatchNormBackward0]
	139944140922624 -> 139944140923824
	139944140922624 [label=ConvolutionBackward0]
	139944140922768 -> 139944140922624
	139944140922768 [label=ReluBackward0]
	139944140922912 -> 139944140922768
	139944140922912 [label=CudnnBatchNormBackward0]
	139944140919552 -> 139944140922912
	139944140919552 [label=ConvolutionBackward0]
	139944140920992 -> 139944140919552
	139944140920992 [label=ReluBackward0]
	139944140926032 -> 139944140920992
	139944140926032 [label=CudnnBatchNormBackward0]
	139944140922336 -> 139944140926032
	139944140922336 [label=ConvolutionBackward0]
	139944140917632 -> 139944140922336
	139944140917632 [label=ReluBackward0]
	139944140918496 -> 139944140917632
	139944140918496 [label=AddBackward0]
	139944140916480 -> 139944140918496
	139944140916480 [label=MulBackward0]
	139944140924112 -> 139944140916480
	139944140924112 [label=CudnnBatchNormBackward0]
	139944140921088 -> 139944140924112
	139944140921088 [label=ConvolutionBackward0]
	139944140921232 -> 139944140921088
	139944140921232 [label=ReluBackward0]
	139944140924928 -> 139944140921232
	139944140924928 [label=CudnnBatchNormBackward0]
	139944140917296 -> 139944140924928
	139944140917296 [label=ConvolutionBackward0]
	139944140920368 -> 139944140917296
	139944140920368 [label=ReluBackward0]
	139944140920416 -> 139944140920368
	139944140920416 [label=CudnnBatchNormBackward0]
	139944140918352 -> 139944140920416
	139944140918352 [label=ConvolutionBackward0]
	139944140927328 -> 139944140918352
	139944140927328 [label=ReluBackward0]
	139944140915664 -> 139944140927328
	139944140915664 [label=AddBackward0]
	139944140916240 -> 139944140915664
	139944140916240 [label=MulBackward0]
	139944140916336 -> 139944140916240
	139944140916336 [label=CudnnBatchNormBackward0]
	139944140917008 -> 139944140916336
	139944140917008 [label=ConvolutionBackward0]
	139944140918112 -> 139944140917008
	139944140918112 [label=ReluBackward0]
	139944140918160 -> 139944140918112
	139944140918160 [label=CudnnBatchNormBackward0]
	139944140918640 -> 139944140918160
	139944140918640 [label=ConvolutionBackward0]
	139944140920560 -> 139944140918640
	139944140920560 [label=ReluBackward0]
	139944140921568 -> 139944140920560
	139944140921568 [label=CudnnBatchNormBackward0]
	139944140921808 -> 139944140921568
	139944140921808 [label=ConvolutionBackward0]
	139944140915280 -> 139944140921808
	139944140915280 [label=ReluBackward0]
	139944140930688 -> 139944140915280
	139944140930688 [label=AddBackward0]
	139944140930928 -> 139944140930688
	139944140930928 [label=MulBackward0]
	139944140927232 -> 139944140930928
	139944140927232 [label=CudnnBatchNormBackward0]
	139944342062512 -> 139944140927232
	139944342062512 [label=ConvolutionBackward0]
	139944342067264 -> 139944342062512
	139944342067264 [label=ReluBackward0]
	139944342066832 -> 139944342067264
	139944342066832 [label=CudnnBatchNormBackward0]
	139944342068560 -> 139944342066832
	139944342068560 [label=ConvolutionBackward0]
	139944342065536 -> 139944342068560
	139944342065536 [label=ReluBackward0]
	139944342070288 -> 139944342065536
	139944342070288 [label=CudnnBatchNormBackward0]
	139944342071152 -> 139944342070288
	139944342071152 [label=ConvolutionBackward0]
	139944475738016 -> 139944342071152
	139944475738016 [label=ReluBackward0]
	139944342073744 -> 139944475738016
	139944342073744 [label=AddBackward0]
	139944342074608 -> 139944342073744
	139944342074608 [label=MulBackward0]
	139944342075904 -> 139944342074608
	139944342075904 [label=CudnnBatchNormBackward0]
	139944342077200 -> 139944342075904
	139944342077200 [label=ConvolutionBackward0]
	139944342063424 -> 139944342077200
	139944342063424 [label=ReluBackward0]
	139944342075376 -> 139944342063424
	139944342075376 [label=CudnnBatchNormBackward0]
	139944342070816 -> 139944342075376
	139944342070816 [label=ConvolutionBackward0]
	139944342067648 -> 139944342070816
	139944342067648 [label=ReluBackward0]
	139944342074656 -> 139944342067648
	139944342074656 [label=CudnnBatchNormBackward0]
	139944342069328 -> 139944342074656
	139944342069328 [label=ConvolutionBackward0]
	139944342074176 -> 139944342069328
	139944342074176 [label=ReluBackward0]
	139944342061216 -> 139944342074176
	139944342061216 [label=AddBackward0]
	139944342063376 -> 139944342061216
	139944342063376 [label=MulBackward0]
	139944140890816 -> 139944342063376
	139944140890816 [label=CudnnBatchNormBackward0]
	139944140882272 -> 139944140890816
	139944140882272 [label=ConvolutionBackward0]
	139944140887408 -> 139944140882272
	139944140887408 [label=ReluBackward0]
	139944140882464 -> 139944140887408
	139944140882464 [label=CudnnBatchNormBackward0]
	139944140891200 -> 139944140882464
	139944140891200 [label=ConvolutionBackward0]
	139944140883664 -> 139944140891200
	139944140883664 [label=ReluBackward0]
	139944140897536 -> 139944140883664
	139944140897536 [label=CudnnBatchNormBackward0]
	139944140884960 -> 139944140897536
	139944140884960 [label=ConvolutionBackward0]
	139944342072064 -> 139944140884960
	139944342072064 [label=ReluBackward0]
	139944140895568 -> 139944342072064
	139944140895568 [label=AddBackward0]
	139944140889088 -> 139944140895568
	139944140889088 [label=MulBackward0]
	139944140893504 -> 139944140889088
	139944140893504 [label=CudnnBatchNormBackward0]
	139944140884288 -> 139944140893504
	139944140884288 [label=ConvolutionBackward0]
	139944140883184 -> 139944140884288
	139944140883184 [label=ReluBackward0]
	139944140887648 -> 139944140883184
	139944140887648 [label=CudnnBatchNormBackward0]
	139944140885296 -> 139944140887648
	139944140885296 [label=ConvolutionBackward0]
	139944140885920 -> 139944140885296
	139944140885920 [label=ReluBackward0]
	139944140886112 -> 139944140885920
	139944140886112 [label=CudnnBatchNormBackward0]
	139944140890624 -> 139944140886112
	139944140890624 [label=ConvolutionBackward0]
	139944140885440 -> 139944140890624
	139944140885440 [label=ReluBackward0]
	139944140886352 -> 139944140885440
	139944140886352 [label=AddBackward0]
	139944140887600 -> 139944140886352
	139944140887600 [label=MulBackward0]
	139944140891392 -> 139944140887600
	139944140891392 [label=CudnnBatchNormBackward0]
	139944140882176 -> 139944140891392
	139944140882176 [label=ConvolutionBackward0]
	139944140889952 -> 139944140882176
	139944140889952 [label=ReluBackward0]
	139944140882800 -> 139944140889952
	139944140882800 [label=CudnnBatchNormBackward0]
	139944140882992 -> 139944140882800
	139944140882992 [label=ConvolutionBackward0]
	139944140884432 -> 139944140882992
	139944140884432 [label=ReluBackward0]
	139944140884816 -> 139944140884432
	139944140884816 [label=CudnnBatchNormBackward0]
	139944140885584 -> 139944140884816
	139944140885584 [label=ConvolutionBackward0]
	139944475728944 -> 139944140885584
	139944475728944 [label=ReluBackward0]
	139944140887312 -> 139944475728944
	139944140887312 [label=AddBackward0]
	139944140887888 -> 139944140887312
	139944140887888 [label=MulBackward0]
	139944140888848 -> 139944140887888
	139944140888848 [label=CudnnBatchNormBackward0]
	139944140896480 -> 139944140888848
	139944140896480 [label=ConvolutionBackward0]
	139944140891344 -> 139944140896480
	139944140891344 [label=ReluBackward0]
	139944140891584 -> 139944140891344
	139944140891584 [label=CudnnBatchNormBackward0]
	139944140890288 -> 139944140891584
	139944140890288 [label=ConvolutionBackward0]
	139944140891008 -> 139944140890288
	139944140891008 [label=ReluBackward0]
	139944140891632 -> 139944140891008
	139944140891632 [label=CudnnBatchNormBackward0]
	139944140891440 -> 139944140891632
	139944140891440 [label=ConvolutionBackward0]
	139944140888176 -> 139944140891440
	139944140888176 [label=ReluBackward0]
	139944140892496 -> 139944140888176
	139944140892496 [label=AddBackward0]
	139944140882944 -> 139944140892496
	139944140882944 [label=MulBackward0]
	139944140883040 -> 139944140882944
	139944140883040 [label=CudnnBatchNormBackward0]
	139944140895760 -> 139944140883040
	139944140895760 [label=ConvolutionBackward0]
	139944140896144 -> 139944140895760
	139944140896144 [label=ReluBackward0]
	139944140884096 -> 139944140896144
	139944140884096 [label=CudnnBatchNormBackward0]
	139944140897296 -> 139944140884096
	139944140897296 [label=ConvolutionBackward0]
	139944140898064 -> 139944140897296
	139944140898064 [label=ReluBackward0]
	139944140898256 -> 139944140898064
	139944140898256 [label=CudnnBatchNormBackward0]
	139944140883328 -> 139944140898256
	139944140883328 [label=ConvolutionBackward0]
	139944140893168 -> 139944140883328
	139944140893168 [label=ReluBackward0]
	139944140884912 -> 139944140893168
	139944140884912 [label=AddBackward0]
	139944140893120 -> 139944140884912
	139944140893120 [label=MulBackward0]
	139944140886304 -> 139944140893120
	139944140886304 [label=CudnnBatchNormBackward0]
	139944140884672 -> 139944140886304
	139944140884672 [label=ConvolutionBackward0]
	139944140897584 -> 139944140884672
	139944140897584 [label=ReluBackward0]
	139944140888608 -> 139944140897584
	139944140888608 [label=CudnnBatchNormBackward0]
	139944140892640 -> 139944140888608
	139944140892640 [label=ConvolutionBackward0]
	139944140895232 -> 139944140892640
	139944140895232 [label=ReluBackward0]
	139944140892448 -> 139944140895232
	139944140892448 [label=CudnnBatchNormBackward0]
	139944140893744 -> 139944140892448
	139944140893744 [label=ConvolutionBackward0]
	139944140895520 -> 139944140893744
	139944140895520 [label=MaxPool2DWithIndicesBackward0]
	139944475532576 -> 139944140895520
	139944475532576 [label=ReluBackward0]
	139944140834224 -> 139944475532576
	139944140834224 [label=CudnnBatchNormBackward0]
	139944140839072 -> 139944140834224
	139944140839072 [label=ConvolutionBackward0]
	139944140834464 -> 139944140839072
	139944341573520 [label="model.model.encoder.layer0.conv1.weight
 (64, 14, 7, 7)" fillcolor=lightblue]
	139944341573520 -> 139944140834464
	139944140834464 [label=AccumulateGrad]
	139944140834320 -> 139944140834224
	139944341575600 [label="model.model.encoder.layer0.bn1.weight
 (64)" fillcolor=lightblue]
	139944341575600 -> 139944140834320
	139944140834320 [label=AccumulateGrad]
	139944140848576 -> 139944140834224
	139944341574320 [label="model.model.encoder.layer0.bn1.bias
 (64)" fillcolor=lightblue]
	139944341574320 -> 139944140848576
	139944140848576 [label=AccumulateGrad]
	139944140883424 -> 139944140893744
	139944140515392 [label="model.model.encoder.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139944140515392 -> 139944140883424
	139944140883424 [label=AccumulateGrad]
	139944140890144 -> 139944140892448
	139944140519072 [label="model.model.encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139944140519072 -> 139944140890144
	139944140890144 [label=AccumulateGrad]
	139944140894464 -> 139944140892448
	139944140516112 [label="model.model.encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139944140516112 -> 139944140894464
	139944140894464 [label=AccumulateGrad]
	139944140892784 -> 139944140892640
	139944140520432 [label="model.model.encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139944140520432 -> 139944140892784
	139944140892784 [label=AccumulateGrad]
	139944140896048 -> 139944140888608
	139944140520192 [label="model.model.encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139944140520192 -> 139944140896048
	139944140896048 [label=AccumulateGrad]
	139944140884144 -> 139944140888608
	139944140520112 [label="model.model.encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139944140520112 -> 139944140884144
	139944140884144 [label=AccumulateGrad]
	139944140897008 -> 139944140884672
	139944140518912 [label="model.model.encoder.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139944140518912 -> 139944140897008
	139944140897008 [label=AccumulateGrad]
	139944140898160 -> 139944140886304
	139944140518592 [label="model.model.encoder.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	139944140518592 -> 139944140898160
	139944140898160 [label=AccumulateGrad]
	139944140897968 -> 139944140886304
	139944140518512 [label="model.model.encoder.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	139944140518512 -> 139944140897968
	139944140897968 [label=AccumulateGrad]
	139944140890336 -> 139944140893120
	139944140890336 [label=SigmoidBackward0]
	139944140895904 -> 139944140890336
	139944140895904 [label=ConvolutionBackward0]
	139944140891968 -> 139944140895904
	139944140891968 [label=ReluBackward0]
	139944140895856 -> 139944140891968
	139944140895856 [label=ConvolutionBackward0]
	139944140834032 -> 139944140895856
	139944140834032 [label=MeanBackward1]
	139944140886304 -> 139944140834032
	139944140840080 -> 139944140895856
	139944140517472 [label="model.model.encoder.layer1.0.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	139944140517472 -> 139944140840080
	139944140840080 [label=AccumulateGrad]
	139944140835760 -> 139944140895856
	139944140517392 [label="model.model.encoder.layer1.0.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	139944140517392 -> 139944140835760
	139944140835760 [label=AccumulateGrad]
	139944140894704 -> 139944140895904
	139944140516992 [label="model.model.encoder.layer1.0.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	139944140516992 -> 139944140894704
	139944140894704 [label=AccumulateGrad]
	139944140892160 -> 139944140895904
	139944140516752 [label="model.model.encoder.layer1.0.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	139944140516752 -> 139944140892160
	139944140892160 [label=AccumulateGrad]
	139944140893024 -> 139944140884912
	139944140893024 [label=CudnnBatchNormBackward0]
	139944140894752 -> 139944140893024
	139944140894752 [label=ConvolutionBackward0]
	139944140895520 -> 139944140894752
	139944140845264 -> 139944140894752
	139944341580240 [label="model.model.encoder.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139944341580240 -> 139944140845264
	139944140845264 [label=AccumulateGrad]
	139944140884384 -> 139944140893024
	139944341575520 [label="model.model.encoder.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139944341575520 -> 139944140884384
	139944140884384 [label=AccumulateGrad]
	139944140893600 -> 139944140893024
	139944341571120 [label="model.model.encoder.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139944341571120 -> 139944140893600
	139944140893600 [label=AccumulateGrad]
	139944140885152 -> 139944140883328
	139944140516352 [label="model.model.encoder.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139944140516352 -> 139944140885152
	139944140885152 [label=AccumulateGrad]
	139944140892064 -> 139944140898256
	139944140516272 [label="model.model.encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139944140516272 -> 139944140892064
	139944140892064 [label=AccumulateGrad]
	139944140897872 -> 139944140898256
	139944140516032 [label="model.model.encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139944140516032 -> 139944140897872
	139944140897872 [label=AccumulateGrad]
	139944140897488 -> 139944140897296
	139944140514832 [label="model.model.encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139944140514832 -> 139944140897488
	139944140897488 [label=AccumulateGrad]
	139944140892592 -> 139944140884096
	139944140514752 [label="model.model.encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139944140514752 -> 139944140892592
	139944140892592 [label=AccumulateGrad]
	139944140896528 -> 139944140884096
	139944140514432 [label="model.model.encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139944140514432 -> 139944140896528
	139944140896528 [label=AccumulateGrad]
	139944140896336 -> 139944140895760
	139944140513312 [label="model.model.encoder.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139944140513312 -> 139944140896336
	139944140896336 [label=AccumulateGrad]
	139944140891776 -> 139944140883040
	139944140513232 [label="model.model.encoder.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	139944140513232 -> 139944140891776
	139944140891776 [label=AccumulateGrad]
	139944140894512 -> 139944140883040
	139944140512912 [label="model.model.encoder.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	139944140512912 -> 139944140894512
	139944140894512 [label=AccumulateGrad]
	139944140894080 -> 139944140882944
	139944140894080 [label=SigmoidBackward0]
	139944140897776 -> 139944140894080
	139944140897776 [label=ConvolutionBackward0]
	139944140896384 -> 139944140897776
	139944140896384 [label=ReluBackward0]
	139944140884000 -> 139944140896384
	139944140884000 [label=ConvolutionBackward0]
	139944140892352 -> 139944140884000
	139944140892352 [label=MeanBackward1]
	139944140883040 -> 139944140892352
	139944140894128 -> 139944140884000
	139944140511712 [label="model.model.encoder.layer1.1.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	139944140511712 -> 139944140894128
	139944140894128 [label=AccumulateGrad]
	139944140894176 -> 139944140884000
	139944140511632 [label="model.model.encoder.layer1.1.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	139944140511632 -> 139944140894176
	139944140894176 [label=AccumulateGrad]
	139944140891824 -> 139944140897776
	139944140511232 [label="model.model.encoder.layer1.1.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	139944140511232 -> 139944140891824
	139944140891824 [label=AccumulateGrad]
	139944140896096 -> 139944140897776
	139944140510912 [label="model.model.encoder.layer1.1.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	139944140510912 -> 139944140896096
	139944140896096 [label=AccumulateGrad]
	139944140893168 -> 139944140892496
	139944140892208 -> 139944140891440
	139944140510512 [label="model.model.encoder.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139944140510512 -> 139944140892208
	139944140892208 [label=AccumulateGrad]
	139944140891680 -> 139944140891632
	139944140510432 [label="model.model.encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	139944140510432 -> 139944140891680
	139944140891680 [label=AccumulateGrad]
	139944140890768 -> 139944140891632
	139944140510112 [label="model.model.encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	139944140510112 -> 139944140890768
	139944140890768 [label=AccumulateGrad]
	139944140890384 -> 139944140890288
	139944140508912 [label="model.model.encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139944140508912 -> 139944140890384
	139944140890384 [label=AccumulateGrad]
	139944140890000 -> 139944140891584
	139944140508832 [label="model.model.encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	139944140508832 -> 139944140890000
	139944140890000 [label=AccumulateGrad]
	139944140882848 -> 139944140891584
	139944140508512 [label="model.model.encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	139944140508512 -> 139944140882848
	139944140882848 [label=AccumulateGrad]
	139944140889424 -> 139944140896480
	139944140507312 [label="model.model.encoder.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139944140507312 -> 139944140889424
	139944140889424 [label=AccumulateGrad]
	139944140889040 -> 139944140888848
	139944140507232 [label="model.model.encoder.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	139944140507232 -> 139944140889040
	139944140889040 [label=AccumulateGrad]
	139944140888752 -> 139944140888848
	139944140506912 [label="model.model.encoder.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	139944140506912 -> 139944140888752
	139944140888752 [label=AccumulateGrad]
	139944140888368 -> 139944140887888
	139944140888368 [label=SigmoidBackward0]
	139944140890480 -> 139944140888368
	139944140890480 [label=ConvolutionBackward0]
	139944140889616 -> 139944140890480
	139944140889616 [label=ReluBackward0]
	139944140894416 -> 139944140889616
	139944140894416 [label=ConvolutionBackward0]
	139944140892688 -> 139944140894416
	139944140892688 [label=MeanBackward1]
	139944140888848 -> 139944140892688
	139944140891728 -> 139944140894416
	139944140505712 [label="model.model.encoder.layer1.2.se_module.fc1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	139944140505712 -> 139944140891728
	139944140891728 [label=AccumulateGrad]
	139944140891920 -> 139944140894416
	139944140505632 [label="model.model.encoder.layer1.2.se_module.fc1.bias
 (16)" fillcolor=lightblue]
	139944140505632 -> 139944140891920
	139944140891920 [label=AccumulateGrad]
	139944140886592 -> 139944140890480
	139944140505232 [label="model.model.encoder.layer1.2.se_module.fc2.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	139944140505232 -> 139944140886592
	139944140886592 [label=AccumulateGrad]
	139944140889232 -> 139944140890480
	139944140505792 [label="model.model.encoder.layer1.2.se_module.fc2.bias
 (256)" fillcolor=lightblue]
	139944140505792 -> 139944140889232
	139944140889232 [label=AccumulateGrad]
	139944140888176 -> 139944140887312
	139944140882752 -> 139944140885584
	139944140514912 [label="model.model.encoder.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	139944140514912 -> 139944140882752
	139944140882752 [label=AccumulateGrad]
	139944140885872 -> 139944140884816
	139944140506272 [label="model.model.encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139944140506272 -> 139944140885872
	139944140885872 [label=AccumulateGrad]
	139944140883280 -> 139944140884816
	139944140513872 [label="model.model.encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139944140513872 -> 139944140883280
	139944140883280 [label=AccumulateGrad]
	139944140883088 -> 139944140882992
	139944140514192 [label="model.model.encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139944140514192 -> 139944140883088
	139944140883088 [label=AccumulateGrad]
	139944140882608 -> 139944140882800
	139944140513072 [label="model.model.encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139944140513072 -> 139944140882608
	139944140882608 [label=AccumulateGrad]
	139944140882512 -> 139944140882800
	139944140495248 [label="model.model.encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139944140495248 -> 139944140882512
	139944140882512 [label=AccumulateGrad]
	139944140891296 -> 139944140882176
	139944140502128 [label="model.model.encoder.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139944140502128 -> 139944140891296
	139944140891296 [label=AccumulateGrad]
	139944140888464 -> 139944140891392
	139944140490448 [label="model.model.encoder.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	139944140490448 -> 139944140888464
	139944140888464 [label=AccumulateGrad]
	139944140884240 -> 139944140891392
	139944140498448 [label="model.model.encoder.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	139944140498448 -> 139944140884240
	139944140884240 [label=AccumulateGrad]
	139944140897152 -> 139944140887600
	139944140897152 [label=SigmoidBackward0]
	139944140897248 -> 139944140897152
	139944140897248 [label=ConvolutionBackward0]
	139944140892832 -> 139944140897248
	139944140892832 [label=ReluBackward0]
	139944140888656 -> 139944140892832
	139944140888656 [label=ConvolutionBackward0]
	139944140895712 -> 139944140888656
	139944140895712 [label=MeanBackward1]
	139944140891392 -> 139944140895712
	139944140885968 -> 139944140888656
	139944140504368 [label="model.model.encoder.layer2.0.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	139944140504368 -> 139944140885968
	139944140885968 [label=AccumulateGrad]
	139944140886160 -> 139944140888656
	139944140504048 [label="model.model.encoder.layer2.0.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	139944140504048 -> 139944140886160
	139944140886160 [label=AccumulateGrad]
	139944140882896 -> 139944140897248
	139944140503648 [label="model.model.encoder.layer2.0.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	139944140503648 -> 139944140882896
	139944140882896 [label=AccumulateGrad]
	139944140889712 -> 139944140897248
	139944140503568 [label="model.model.encoder.layer2.0.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	139944140503568 -> 139944140889712
	139944140889712 [label=AccumulateGrad]
	139944140889520 -> 139944140886352
	139944140889520 [label=CudnnBatchNormBackward0]
	139944140887120 -> 139944140889520
	139944140887120 [label=ConvolutionBackward0]
	139944475728944 -> 139944140887120
	139944140891056 -> 139944140887120
	139944140515712 [label="model.model.encoder.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139944140515712 -> 139944140891056
	139944140891056 [label=AccumulateGrad]
	139944140882368 -> 139944140889520
	139944140513472 [label="model.model.encoder.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139944140513472 -> 139944140882368
	139944140882368 [label=AccumulateGrad]
	139944140890864 -> 139944140889520
	139944140505872 [label="model.model.encoder.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139944140505872 -> 139944140890864
	139944140890864 [label=AccumulateGrad]
	139944140887168 -> 139944140890624
	139944140503168 [label="model.model.encoder.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139944140503168 -> 139944140887168
	139944140887168 [label=AccumulateGrad]
	139944140886880 -> 139944140886112
	139944140502848 [label="model.model.encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139944140502848 -> 139944140886880
	139944140886880 [label=AccumulateGrad]
	139944140885728 -> 139944140886112
	139944140502768 [label="model.model.encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139944140502768 -> 139944140885728
	139944140885728 [label=AccumulateGrad]
	139944140889568 -> 139944140885296
	139944140501568 [label="model.model.encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139944140501568 -> 139944140889568
	139944140889568 [label=AccumulateGrad]
	139944140884336 -> 139944140887648
	139944140501248 [label="model.model.encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139944140501248 -> 139944140884336
	139944140884336 [label=AccumulateGrad]
	139944140894992 -> 139944140887648
	139944140501168 [label="model.model.encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139944140501168 -> 139944140894992
	139944140894992 [label=AccumulateGrad]
	139944140883616 -> 139944140884288
	139944140500128 [label="model.model.encoder.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139944140500128 -> 139944140883616
	139944140883616 [label=AccumulateGrad]
	139944140889376 -> 139944140893504
	139944140499888 [label="model.model.encoder.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	139944140499888 -> 139944140889376
	139944140889376 [label=AccumulateGrad]
	139944140885632 -> 139944140893504
	139944140499808 [label="model.model.encoder.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	139944140499808 -> 139944140885632
	139944140885632 [label=AccumulateGrad]
	139944140896816 -> 139944140889088
	139944140896816 [label=SigmoidBackward0]
	139944140885536 -> 139944140896816
	139944140885536 [label=ConvolutionBackward0]
	139944140889904 -> 139944140885536
	139944140889904 [label=ReluBackward0]
	139944140890672 -> 139944140889904
	139944140890672 [label=ConvolutionBackward0]
	139944140895376 -> 139944140890672
	139944140895376 [label=MeanBackward1]
	139944140893504 -> 139944140895376
	139944140891536 -> 139944140890672
	139944140498608 [label="model.model.encoder.layer2.1.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	139944140498608 -> 139944140891536
	139944140891536 [label=AccumulateGrad]
	139944140893408 -> 139944140890672
	139944140498288 [label="model.model.encoder.layer2.1.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	139944140498288 -> 139944140893408
	139944140893408 [label=AccumulateGrad]
	139944140884480 -> 139944140885536
	139944140497888 [label="model.model.encoder.layer2.1.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	139944140497888 -> 139944140884480
	139944140884480 [label=AccumulateGrad]
	139944140881984 -> 139944140885536
	139944140497808 [label="model.model.encoder.layer2.1.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	139944140497808 -> 139944140881984
	139944140881984 [label=AccumulateGrad]
	139944140885440 -> 139944140895568
	139944140888992 -> 139944140884960
	139944140497408 [label="model.model.encoder.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139944140497408 -> 139944140888992
	139944140888992 [label=AccumulateGrad]
	139944140882032 -> 139944140897536
	139944140497088 [label="model.model.encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	139944140497088 -> 139944140882032
	139944140882032 [label=AccumulateGrad]
	139944140884864 -> 139944140897536
	139944140497008 [label="model.model.encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	139944140497008 -> 139944140884864
	139944140884864 [label=AccumulateGrad]
	139944140898112 -> 139944140891200
	139944140495808 [label="model.model.encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139944140495808 -> 139944140898112
	139944140898112 [label=AccumulateGrad]
	139944140888224 -> 139944140882464
	139944140495568 [label="model.model.encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	139944140495568 -> 139944140888224
	139944140888224 [label=AccumulateGrad]
	139944140886448 -> 139944140882464
	139944140495488 [label="model.model.encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	139944140495488 -> 139944140886448
	139944140886448 [label=AccumulateGrad]
	139944140896432 -> 139944140882272
	139944140494288 [label="model.model.encoder.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139944140494288 -> 139944140896432
	139944140896432 [label=AccumulateGrad]
	139944140885008 -> 139944140890816
	139944140494048 [label="model.model.encoder.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	139944140494048 -> 139944140885008
	139944140885008 [label=AccumulateGrad]
	139944140897200 -> 139944140890816
	139944140493968 [label="model.model.encoder.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	139944140493968 -> 139944140897200
	139944140897200 [label=AccumulateGrad]
	139944140897104 -> 139944342063376
	139944140897104 [label=SigmoidBackward0]
	139944140886208 -> 139944140897104
	139944140886208 [label=ConvolutionBackward0]
	139944140895088 -> 139944140886208
	139944140895088 [label=ReluBackward0]
	139944140889184 -> 139944140895088
	139944140889184 [label=ConvolutionBackward0]
	139944140897728 -> 139944140889184
	139944140897728 [label=MeanBackward1]
	139944140890816 -> 139944140897728
	139944140882416 -> 139944140889184
	139944140492928 [label="model.model.encoder.layer2.2.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	139944140492928 -> 139944140882416
	139944140882416 [label=AccumulateGrad]
	139944140890432 -> 139944140889184
	139944140492608 [label="model.model.encoder.layer2.2.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	139944140492608 -> 139944140890432
	139944140890432 [label=AccumulateGrad]
	139944140897632 -> 139944140886208
	139944140492288 [label="model.model.encoder.layer2.2.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	139944140492288 -> 139944140897632
	139944140897632 [label=AccumulateGrad]
	139944140888704 -> 139944140886208
	139944140492208 [label="model.model.encoder.layer2.2.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	139944140492208 -> 139944140888704
	139944140888704 [label=AccumulateGrad]
	139944342072064 -> 139944342061216
	139944342076480 -> 139944342069328
	139944140491808 [label="model.model.encoder.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139944140491808 -> 139944342076480
	139944342076480 [label=AccumulateGrad]
	139944342064624 -> 139944342074656
	139944140491488 [label="model.model.encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	139944140491488 -> 139944342064624
	139944342064624 [label=AccumulateGrad]
	139944342061408 -> 139944342074656
	139944140491408 [label="model.model.encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	139944140491408 -> 139944342061408
	139944342061408 [label=AccumulateGrad]
	139944342073120 -> 139944342070816
	139944140490208 [label="model.model.encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139944140490208 -> 139944342073120
	139944342073120 [label=AccumulateGrad]
	139944342070960 -> 139944342075376
	139944140489888 [label="model.model.encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	139944140489888 -> 139944342070960
	139944342070960 [label=AccumulateGrad]
	139944342061312 -> 139944342075376
	139944140489808 [label="model.model.encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	139944140489808 -> 139944342061312
	139944342061312 [label=AccumulateGrad]
	139944342067744 -> 139944342077200
	139944140488768 [label="model.model.encoder.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139944140488768 -> 139944342067744
	139944342067744 [label=AccumulateGrad]
	139944342076768 -> 139944342075904
	139944140496448 [label="model.model.encoder.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	139944140496448 -> 139944342076768
	139944342076768 [label=AccumulateGrad]
	139944342076336 -> 139944342075904
	139944140493808 [label="model.model.encoder.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	139944140493808 -> 139944342076336
	139944342076336 [label=AccumulateGrad]
	139944342075472 -> 139944342074608
	139944342075472 [label=SigmoidBackward0]
	139944342068368 -> 139944342075472
	139944342068368 [label=ConvolutionBackward0]
	139944342070624 -> 139944342068368
	139944342070624 [label=ReluBackward0]
	139944342072928 -> 139944342070624
	139944342072928 [label=ConvolutionBackward0]
	139944342069952 -> 139944342072928
	139944342069952 [label=MeanBackward1]
	139944342075904 -> 139944342069952
	139944140893696 -> 139944342072928
	139944140501808 [label="model.model.encoder.layer2.3.se_module.fc1.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	139944140501808 -> 139944140893696
	139944140893696 [label=AccumulateGrad]
	139944140897680 -> 139944342072928
	139944140492448 [label="model.model.encoder.layer2.3.se_module.fc1.bias
 (32)" fillcolor=lightblue]
	139944140492448 -> 139944140897680
	139944140897680 [label=AccumulateGrad]
	139944342068800 -> 139944342068368
	139944140496768 [label="model.model.encoder.layer2.3.se_module.fc2.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	139944140496768 -> 139944342068800
	139944342068800 [label=AccumulateGrad]
	139944342076384 -> 139944342068368
	139944140503728 [label="model.model.encoder.layer2.3.se_module.fc2.bias
 (512)" fillcolor=lightblue]
	139944140503728 -> 139944342076384
	139944342076384 [label=AccumulateGrad]
	139944342074176 -> 139944342073744
	139944342072880 -> 139944342071152
	139944140491648 [label="model.model.encoder.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	139944140491648 -> 139944342072880
	139944342072880 [label=AccumulateGrad]
	139944342070720 -> 139944342070288
	139944140495968 [label="model.model.encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139944140495968 -> 139944342070720
	139944342070720 [label=AccumulateGrad]
	139944342069424 -> 139944342070288
	139944140498768 [label="model.model.encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139944140498768 -> 139944342069424
	139944342069424 [label=AccumulateGrad]
	139944342064672 -> 139944342068560
	139944341404256 [label="model.model.encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944341404256 -> 139944342064672
	139944342064672 [label=AccumulateGrad]
	139944342065104 -> 139944342066832
	139944341398976 [label="model.model.encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139944341398976 -> 139944342065104
	139944342065104 [label=AccumulateGrad]
	139944342068992 -> 139944342066832
	139944341404496 [label="model.model.encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139944341404496 -> 139944342068992
	139944342068992 [label=AccumulateGrad]
	139944342065968 -> 139944342062512
	139944341404416 [label="model.model.encoder.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944341404416 -> 139944342065968
	139944342065968 [label=AccumulateGrad]
	139944342063808 -> 139944140927232
	139944341401536 [label="model.model.encoder.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	139944341401536 -> 139944342063808
	139944342063808 [label=AccumulateGrad]
	139944342064240 -> 139944140927232
	139944341400736 [label="model.model.encoder.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	139944341400736 -> 139944342064240
	139944342064240 [label=AccumulateGrad]
	139944140919072 -> 139944140930928
	139944140919072 [label=SigmoidBackward0]
	139944342068128 -> 139944140919072
	139944342068128 [label=ConvolutionBackward0]
	139944342066400 -> 139944342068128
	139944342066400 [label=ReluBackward0]
	139944342075040 -> 139944342066400
	139944342075040 [label=ConvolutionBackward0]
	139944342073312 -> 139944342075040
	139944342073312 [label=MeanBackward1]
	139944140927232 -> 139944342073312
	139944342072016 -> 139944342075040
	139944341402256 [label="model.model.encoder.layer3.0.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944341402256 -> 139944342072016
	139944342072016 [label=AccumulateGrad]
	139944342071584 -> 139944342075040
	139944341401216 [label="model.model.encoder.layer3.0.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944341401216 -> 139944342071584
	139944342071584 [label=AccumulateGrad]
	139944342067696 -> 139944342068128
	139944341400816 [label="model.model.encoder.layer3.0.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944341400816 -> 139944342067696
	139944342067696 [label=AccumulateGrad]
	139944342062944 -> 139944342068128
	139944341400336 [label="model.model.encoder.layer3.0.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944341400336 -> 139944342062944
	139944342062944 [label=AccumulateGrad]
	139944140925264 -> 139944140930688
	139944140925264 [label=CudnnBatchNormBackward0]
	139944342072448 -> 139944140925264
	139944342072448 [label=ConvolutionBackward0]
	139944475738016 -> 139944342072448
	139944342075184 -> 139944342072448
	139944140504208 [label="model.model.encoder.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139944140504208 -> 139944342075184
	139944342075184 [label=AccumulateGrad]
	139944342061648 -> 139944140925264
	139944140504608 [label="model.model.encoder.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	139944140504608 -> 139944342061648
	139944342061648 [label=AccumulateGrad]
	139944342062080 -> 139944140925264
	139944140491568 [label="model.model.encoder.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	139944140491568 -> 139944342062080
	139944342062080 [label=AccumulateGrad]
	139944140923536 -> 139944140921808
	139944341405456 [label="model.model.encoder.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139944341405456 -> 139944140923536
	139944140923536 [label=AccumulateGrad]
	139944140921040 -> 139944140921568
	139944341404736 [label="model.model.encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139944341404736 -> 139944140921040
	139944140921040 [label=AccumulateGrad]
	139944140920656 -> 139944140921568
	139944341405296 [label="model.model.encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139944341405296 -> 139944140920656
	139944140920656 [label=AccumulateGrad]
	139944140920464 -> 139944140918640
	139944341404336 [label="model.model.encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944341404336 -> 139944140920464
	139944140920464 [label=AccumulateGrad]
	139944140918928 -> 139944140918160
	139944341401856 [label="model.model.encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139944341401856 -> 139944140918928
	139944140918928 [label=AccumulateGrad]
	139944140919264 -> 139944140918160
	139944341400176 [label="model.model.encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139944341400176 -> 139944140919264
	139944140919264 [label=AccumulateGrad]
	139944140917200 -> 139944140917008
	139944341402896 [label="model.model.encoder.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944341402896 -> 139944140917200
	139944140917200 [label=AccumulateGrad]
	139944140930880 -> 139944140916336
	139944341403136 [label="model.model.encoder.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	139944341403136 -> 139944140930880
	139944140930880 [label=AccumulateGrad]
	139944140916624 -> 139944140916336
	139944341402816 [label="model.model.encoder.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	139944341402816 -> 139944140916624
	139944140916624 [label=AccumulateGrad]
	139944140916384 -> 139944140916240
	139944140916384 [label=SigmoidBackward0]
	139944140919120 -> 139944140916384
	139944140919120 [label=ConvolutionBackward0]
	139944140918304 -> 139944140919120
	139944140918304 [label=ReluBackward0]
	139944140922960 -> 139944140918304
	139944140922960 [label=ConvolutionBackward0]
	139944140923344 -> 139944140922960
	139944140923344 [label=MeanBackward1]
	139944140916336 -> 139944140923344
	139944140921616 -> 139944140922960
	139944140124496 [label="model.model.encoder.layer3.1.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944140124496 -> 139944140921616
	139944140921616 [label=AccumulateGrad]
	139944342069856 -> 139944140922960
	139944140127456 [label="model.model.encoder.layer3.1.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944140127456 -> 139944342069856
	139944342069856 [label=AccumulateGrad]
	139944140919744 -> 139944140919120
	139944140127616 [label="model.model.encoder.layer3.1.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944140127616 -> 139944140919744
	139944140919744 [label=AccumulateGrad]
	139944140916816 -> 139944140919120
	139944140125776 [label="model.model.encoder.layer3.1.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944140125776 -> 139944140916816
	139944140916816 [label=AccumulateGrad]
	139944140915280 -> 139944140915664
	139944140915184 -> 139944140918352
	139944140125856 [label="model.model.encoder.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139944140125856 -> 139944140915184
	139944140915184 [label=AccumulateGrad]
	139944140921376 -> 139944140920416
	139944140125296 [label="model.model.encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	139944140125296 -> 139944140921376
	139944140921376 [label=AccumulateGrad]
	139944140924976 -> 139944140920416
	139944140125376 [label="model.model.encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	139944140125376 -> 139944140924976
	139944140924976 [label=AccumulateGrad]
	139944140923104 -> 139944140917296
	139944140127296 [label="model.model.encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944140127296 -> 139944140923104
	139944140923104 [label=AccumulateGrad]
	139944140918064 -> 139944140924928
	139944140126976 [label="model.model.encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	139944140126976 -> 139944140918064
	139944140918064 [label=AccumulateGrad]
	139944140919888 -> 139944140924928
	139944140127936 [label="model.model.encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	139944140127936 -> 139944140919888
	139944140919888 [label=AccumulateGrad]
	139944140921136 -> 139944140921088
	139944140126576 [label="model.model.encoder.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944140126576 -> 139944140921136
	139944140921136 [label=AccumulateGrad]
	139944140918880 -> 139944140924112
	139944140123536 [label="model.model.encoder.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	139944140123536 -> 139944140918880
	139944140918880 [label=AccumulateGrad]
	139944140920752 -> 139944140924112
	139944140126096 [label="model.model.encoder.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	139944140126096 -> 139944140920752
	139944140920752 [label=AccumulateGrad]
	139944140920272 -> 139944140916480
	139944140920272 [label=SigmoidBackward0]
	139944140917824 -> 139944140920272
	139944140917824 [label=ConvolutionBackward0]
	139944140917056 -> 139944140917824
	139944140917056 [label=ReluBackward0]
	139944140916096 -> 139944140917056
	139944140916096 [label=ConvolutionBackward0]
	139944140915088 -> 139944140916096
	139944140915088 [label=MeanBackward1]
	139944140924112 -> 139944140915088
	139944140917968 -> 139944140916096
	139944140366736 [label="model.model.encoder.layer3.2.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944140366736 -> 139944140917968
	139944140917968 [label=AccumulateGrad]
	139944140925168 -> 139944140916096
	139944140371696 [label="model.model.encoder.layer3.2.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944140371696 -> 139944140925168
	139944140925168 [label=AccumulateGrad]
	139944140922720 -> 139944140917824
	139944140358496 [label="model.model.encoder.layer3.2.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944140358496 -> 139944140922720
	139944140922720 [label=AccumulateGrad]
	139944140920608 -> 139944140917824
	139944140366896 [label="model.model.encoder.layer3.2.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944140366896 -> 139944140920608
	139944140920608 [label=AccumulateGrad]
	139944140927328 -> 139944140918496
	139944140923296 -> 139944140922336
	139944140373056 [label="model.model.encoder.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139944140373056 -> 139944140923296
	139944140923296 [label=AccumulateGrad]
	139944140915904 -> 139944140926032
	139944140373136 [label="model.model.encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	139944140373136 -> 139944140915904
	139944140915904 [label=AccumulateGrad]
	139944140929728 -> 139944140926032
	139944140372736 [label="model.model.encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	139944140372736 -> 139944140929728
	139944140929728 [label=AccumulateGrad]
	139944140925120 -> 139944140919552
	139944140371536 [label="model.model.encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944140371536 -> 139944140925120
	139944140925120 [label=AccumulateGrad]
	139944140923584 -> 139944140922912
	139944140371376 [label="model.model.encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	139944140371376 -> 139944140923584
	139944140923584 [label=AccumulateGrad]
	139944140922576 -> 139944140922912
	139944140371056 [label="model.model.encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	139944140371056 -> 139944140922576
	139944140922576 [label=AccumulateGrad]
	139944140920080 -> 139944140922624
	139944140369776 [label="model.model.encoder.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944140369776 -> 139944140920080
	139944140920080 [label=AccumulateGrad]
	139944140922192 -> 139944140923824
	139944140369696 [label="model.model.encoder.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	139944140369696 -> 139944140922192
	139944140922192 [label=AccumulateGrad]
	139944140921424 -> 139944140923824
	139944140369536 [label="model.model.encoder.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	139944140369536 -> 139944140921424
	139944140921424 [label=AccumulateGrad]
	139944140919216 -> 139944140918688
	139944140919216 [label=SigmoidBackward0]
	139944342068608 -> 139944140919216
	139944342068608 [label=ConvolutionBackward0]
	139944140923200 -> 139944342068608
	139944140923200 [label=ReluBackward0]
	139944140926368 -> 139944140923200
	139944140926368 [label=ConvolutionBackward0]
	139944140926896 -> 139944140926368
	139944140926896 [label=MeanBackward1]
	139944140923824 -> 139944140926896
	139944140930832 -> 139944140926368
	139944140368176 [label="model.model.encoder.layer3.3.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944140368176 -> 139944140930832
	139944140930832 [label=AccumulateGrad]
	139944140919648 -> 139944140926368
	139944140368096 [label="model.model.encoder.layer3.3.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944140368096 -> 139944140919648
	139944140919648 [label=AccumulateGrad]
	139944140924352 -> 139944342068608
	139944140367776 [label="model.model.encoder.layer3.3.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944140367776 -> 139944140924352
	139944140924352 [label=AccumulateGrad]
	139944140919312 -> 139944342068608
	139944140367376 [label="model.model.encoder.layer3.3.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944140367376 -> 139944140919312
	139944140919312 [label=AccumulateGrad]
	139944140917632 -> 139944140918976
	139944140922144 -> 139944140914752
	139944140367056 [label="model.model.encoder.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139944140367056 -> 139944140922144
	139944140922144 [label=AccumulateGrad]
	139944140927856 -> 139944140926800
	139944140366976 [label="model.model.encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	139944140366976 -> 139944140927856
	139944140927856 [label=AccumulateGrad]
	139944140924448 -> 139944140926800
	139944140366576 [label="model.model.encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	139944140366576 -> 139944140924448
	139944140924448 [label=AccumulateGrad]
	139944140925936 -> 139944140922816
	139944140365456 [label="model.model.encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944140365456 -> 139944140925936
	139944140925936 [label=AccumulateGrad]
	139944140924208 -> 139944140925456
	139944140365376 [label="model.model.encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	139944140365376 -> 139944140924208
	139944140924208 [label=AccumulateGrad]
	139944140927904 -> 139944140925456
	139944140364976 [label="model.model.encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	139944140364976 -> 139944140927904
	139944140927904 [label=AccumulateGrad]
	139944140923008 -> 139944177607216
	139944140364096 [label="model.model.encoder.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944140364096 -> 139944140923008
	139944140923008 [label=AccumulateGrad]
	139944177607264 -> 139944177608224
	139944140363856 [label="model.model.encoder.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	139944140363856 -> 139944177607264
	139944177607264 [label=AccumulateGrad]
	139944140918256 -> 139944177608224
	139944140363616 [label="model.model.encoder.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	139944140363616 -> 139944140918256
	139944140918256 [label=AccumulateGrad]
	139944177606832 -> 139944177610624
	139944177606832 [label=SigmoidBackward0]
	139944140925984 -> 139944177606832
	139944140925984 [label=ConvolutionBackward0]
	139944140921472 -> 139944140925984
	139944140921472 [label=ReluBackward0]
	139944140918448 -> 139944140921472
	139944140918448 [label=ConvolutionBackward0]
	139944140921760 -> 139944140918448
	139944140921760 [label=MeanBackward1]
	139944177608224 -> 139944140921760
	139944140925408 -> 139944140918448
	139944140362736 [label="model.model.encoder.layer3.4.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944140362736 -> 139944140925408
	139944140925408 [label=AccumulateGrad]
	139944140919408 -> 139944140918448
	139944140362656 [label="model.model.encoder.layer3.4.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944140362656 -> 139944140919408
	139944140919408 [label=AccumulateGrad]
	139944140927040 -> 139944140925984
	139944140362016 [label="model.model.encoder.layer3.4.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944140362016 -> 139944140927040
	139944140927040 [label=AccumulateGrad]
	139944140928000 -> 139944140925984
	139944140361936 [label="model.model.encoder.layer3.4.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944140361936 -> 139944140928000
	139944140928000 [label=AccumulateGrad]
	139944177606784 -> 139944177608320
	139944140783776 -> 139944140783920
	139944140361296 [label="model.model.encoder.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139944140361296 -> 139944140783776
	139944140783776 [label=AccumulateGrad]
	139944140783824 -> 139944140784064
	139944140361136 [label="model.model.encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	139944140361136 -> 139944140783824
	139944140783824 [label=AccumulateGrad]
	139944140784208 -> 139944140784064
	139944140361216 [label="model.model.encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	139944140361216 -> 139944140784208
	139944140784208 [label=AccumulateGrad]
	139944140784352 -> 139944140784448
	139944140359936 [label="model.model.encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944140359936 -> 139944140784352
	139944140784352 [label=AccumulateGrad]
	139944140784640 -> 139944140784592
	139944140357696 [label="model.model.encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	139944140357696 -> 139944140784640
	139944140784640 [label=AccumulateGrad]
	139944140785024 -> 139944140784592
	139944140358416 [label="model.model.encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	139944140358416 -> 139944140785024
	139944140785024 [label=AccumulateGrad]
	139944140785168 -> 139944140783680
	139944140359056 [label="model.model.encoder.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139944140359056 -> 139944140785168
	139944140785168 [label=AccumulateGrad]
	139944140785456 -> 139944140785600
	139944140366816 [label="model.model.encoder.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	139944140366816 -> 139944140785456
	139944140785456 [label=AccumulateGrad]
	139944140783872 -> 139944140785600
	139944140369616 [label="model.model.encoder.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	139944140369616 -> 139944140783872
	139944140783872 [label=AccumulateGrad]
	139944140783968 -> 139944140784112
	139944140783968 [label=SigmoidBackward0]
	139944140784304 -> 139944140783968
	139944140784304 [label=ConvolutionBackward0]
	139944140784832 -> 139944140784304
	139944140784832 [label=ReluBackward0]
	139944140784016 -> 139944140784832
	139944140784016 [label=ConvolutionBackward0]
	139944177610432 -> 139944140784016
	139944177610432 [label=MeanBackward1]
	139944140785600 -> 139944177610432
	139944177611248 -> 139944140784016
	139944140362336 [label="model.model.encoder.layer3.5.se_module.fc1.weight
 (64, 1024, 1, 1)" fillcolor=lightblue]
	139944140362336 -> 139944177611248
	139944177611248 [label=AccumulateGrad]
	139944177610336 -> 139944140784016
	139944140364256 [label="model.model.encoder.layer3.5.se_module.fc1.bias
 (64)" fillcolor=lightblue]
	139944140364256 -> 139944177610336
	139944177610336 [label=AccumulateGrad]
	139944140784496 -> 139944140784304
	139944140370816 [label="model.model.encoder.layer3.5.se_module.fc2.weight
 (1024, 64, 1, 1)" fillcolor=lightblue]
	139944140370816 -> 139944140784496
	139944140784496 [label=AccumulateGrad]
	139944140785312 -> 139944140784304
	139944140368736 [label="model.model.encoder.layer3.5.se_module.fc2.bias
 (1024)" fillcolor=lightblue]
	139944140368736 -> 139944140785312
	139944140785312 [label=AccumulateGrad]
	139944140785888 -> 139944140784256
	139944140784400 -> 139944140786176
	139944140368496 [label="model.model.encoder.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	139944140368496 -> 139944140784400
	139944140784400 [label=AccumulateGrad]
	139944140784928 -> 139944140786320
	139944140358976 [label="model.model.encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139944140358976 -> 139944140784928
	139944140784928 [label=AccumulateGrad]
	139944140787136 -> 139944140786320
	139944140369856 [label="model.model.encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139944140369856 -> 139944140787136
	139944140787136 [label=AccumulateGrad]
	139944140785984 -> 139944140795440
	139944140781616 [label="model.model.encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139944140781616 -> 139944140785984
	139944140785984 [label=AccumulateGrad]
	139944140796592 -> 139944140796208
	139944140781696 [label="model.model.encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139944140781696 -> 139944140796592
	139944140796592 [label=AccumulateGrad]
	139944140795248 -> 139944140796208
	139944140781456 [label="model.model.encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139944140781456 -> 139944140795248
	139944140795248 [label=AccumulateGrad]
	139944140794288 -> 139944140793712
	139944140780736 [label="model.model.encoder.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139944140780736 -> 139944140794288
	139944140794288 [label=AccumulateGrad]
	139944140789872 -> 139944140799952
	139944140782656 [label="model.model.encoder.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	139944140782656 -> 139944140789872
	139944140789872 [label=AccumulateGrad]
	139944140797360 -> 139944140799952
	139944140780336 [label="model.model.encoder.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	139944140780336 -> 139944140797360
	139944140797360 [label=AccumulateGrad]
	139944140789488 -> 139944140799904
	139944140789488 [label=SigmoidBackward0]
	139944140796736 -> 139944140789488
	139944140796736 [label=ConvolutionBackward0]
	139944140793520 -> 139944140796736
	139944140793520 [label=ReluBackward0]
	139944140785744 -> 139944140793520
	139944140785744 [label=ConvolutionBackward0]
	139944140786032 -> 139944140785744
	139944140786032 [label=MeanBackward1]
	139944140799952 -> 139944140786032
	139944140784544 -> 139944140785744
	139944140780656 [label="model.model.encoder.layer4.0.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	139944140780656 -> 139944140784544
	139944140784544 [label=AccumulateGrad]
	139944140784784 -> 139944140785744
	139944140778896 [label="model.model.encoder.layer4.0.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	139944140778896 -> 139944140784784
	139944140784784 [label=AccumulateGrad]
	139944140798944 -> 139944140796736
	139944140770336 [label="model.model.encoder.layer4.0.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	139944140770336 -> 139944140798944
	139944140798944 [label=AccumulateGrad]
	139944140793328 -> 139944140796736
	139944140781056 [label="model.model.encoder.layer4.0.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	139944140781056 -> 139944140793328
	139944140793328 [label=AccumulateGrad]
	139944140799616 -> 139944140799760
	139944140799616 [label=CudnnBatchNormBackward0]
	139944177610672 -> 139944140799616
	139944177610672 [label=ConvolutionBackward0]
	139944139990944 -> 139944177610672
	139944140783728 -> 139944177610672
	139944140366336 [label="model.model.encoder.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139944140366336 -> 139944140783728
	139944140783728 [label=AccumulateGrad]
	139944140797888 -> 139944140799616
	139944140364416 [label="model.model.encoder.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	139944140364416 -> 139944140797888
	139944140797888 [label=AccumulateGrad]
	139944140799856 -> 139944140799616
	139944140365296 [label="model.model.encoder.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	139944140365296 -> 139944140799856
	139944140799856 [label=AccumulateGrad]
	139944140799376 -> 139944140799136
	139944140783216 [label="model.model.encoder.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139944140783216 -> 139944140799376
	139944140799376 [label=AccumulateGrad]
	139944140798896 -> 139944140798992
	139944140782576 [label="model.model.encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139944140782576 -> 139944140798896
	139944140798896 [label=AccumulateGrad]
	139944140798848 -> 139944140798992
	139944140782496 [label="model.model.encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139944140782496 -> 139944140798848
	139944140798848 [label=AccumulateGrad]
	139944140798704 -> 139944140798368
	139943214127584 [label="model.model.encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139943214127584 -> 139944140798704
	139944140798704 [label=AccumulateGrad]
	139944140798416 -> 139944140798272
	139943214136224 [label="model.model.encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139943214136224 -> 139944140798416
	139944140798416 [label=AccumulateGrad]
	139944140798656 -> 139944140798272
	139943214121104 [label="model.model.encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139943214121104 -> 139944140798656
	139944140798656 [label=AccumulateGrad]
	139944140799472 -> 139944140799808
	139943214136304 [label="model.model.encoder.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139943214136304 -> 139944140799472
	139944140799472 [label=AccumulateGrad]
	139944140797504 -> 139944140786560
	139943214133264 [label="model.model.encoder.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	139943214133264 -> 139944140797504
	139944140797504 [label=AccumulateGrad]
	139944140797552 -> 139944140786560
	139943214128064 [label="model.model.encoder.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	139943214128064 -> 139944140797552
	139944140797552 [label=AccumulateGrad]
	139944140796784 -> 139944140795344
	139944140796784 [label=SigmoidBackward0]
	139944140798464 -> 139944140796784
	139944140798464 [label=ConvolutionBackward0]
	139944140798320 -> 139944140798464
	139944140798320 [label=ReluBackward0]
	139944140785840 -> 139944140798320
	139944140785840 [label=ConvolutionBackward0]
	139944140799328 -> 139944140785840
	139944140799328 [label=MeanBackward1]
	139944140786560 -> 139944140799328
	139944140788960 -> 139944140785840
	139943214127264 [label="model.model.encoder.layer4.1.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	139943214127264 -> 139944140788960
	139944140788960 [label=AccumulateGrad]
	139944140788912 -> 139944140785840
	139943214129424 [label="model.model.encoder.layer4.1.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	139943214129424 -> 139944140788912
	139944140788912 [label=AccumulateGrad]
	139944140798512 -> 139944140798464
	139943214127904 [label="model.model.encoder.layer4.1.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	139943214127904 -> 139944140798512
	139944140798512 [label=AccumulateGrad]
	139944140793472 -> 139944140798464
	139943214123024 [label="model.model.encoder.layer4.1.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	139943214123024 -> 139944140793472
	139944140793472 [label=AccumulateGrad]
	139944140790928 -> 139944140786224
	139944140787760 -> 139944140799664
	139944341651760 [label="model.model.encoder.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139944341651760 -> 139944140787760
	139944140787760 [label=AccumulateGrad]
	139944140794624 -> 139944139987008
	139944341657520 [label="model.model.encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	139944341657520 -> 139944140794624
	139944140794624 [label=AccumulateGrad]
	139944140798752 -> 139944139987008
	139944341652800 [label="model.model.encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	139944341652800 -> 139944140798752
	139944140798752 [label=AccumulateGrad]
	139944139986432 -> 139944139992912
	139944341667760 [label="model.model.encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139944341667760 -> 139944139986432
	139944139986432 [label=AccumulateGrad]
	139944139988832 -> 139944139993488
	139944341667680 [label="model.model.encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	139944341667680 -> 139944139988832
	139944139988832 [label=AccumulateGrad]
	139944139993440 -> 139944139993488
	139944341667600 [label="model.model.encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	139944341667600 -> 139944139993440
	139944139993440 [label=AccumulateGrad]
	139944139986912 -> 139944139996272
	139944341667200 [label="model.model.encoder.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139944341667200 -> 139944139986912
	139944139986912 [label=AccumulateGrad]
	139944139986864 -> 139944139994928
	139944341666960 [label="model.model.encoder.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	139944341666960 -> 139944139986864
	139944139986864 [label=AccumulateGrad]
	139944139991184 -> 139944139994928
	139944341666880 [label="model.model.encoder.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	139944341666880 -> 139944139991184
	139944139991184 [label=AccumulateGrad]
	139944139992336 -> 139944139992720
	139944139992336 [label=SigmoidBackward0]
	139944139993536 -> 139944139992336
	139944139993536 [label=ConvolutionBackward0]
	139944139988064 -> 139944139993536
	139944139988064 [label=ReluBackward0]
	139944140797456 -> 139944139988064
	139944140797456 [label=ConvolutionBackward0]
	139944140787664 -> 139944140797456
	139944140787664 [label=MeanBackward1]
	139944139994928 -> 139944140787664
	139944140794720 -> 139944140797456
	139944341666080 [label="model.model.encoder.layer4.2.se_module.fc1.weight
 (128, 2048, 1, 1)" fillcolor=lightblue]
	139944341666080 -> 139944140794720
	139944140794720 [label=AccumulateGrad]
	139944140790448 -> 139944140797456
	139944341666000 [label="model.model.encoder.layer4.2.se_module.fc1.bias
 (128)" fillcolor=lightblue]
	139944341666000 -> 139944140790448
	139944140790448 [label=AccumulateGrad]
	139944139995024 -> 139944139993536
	139944341665680 [label="model.model.encoder.layer4.2.se_module.fc2.weight
 (2048, 128, 1, 1)" fillcolor=lightblue]
	139944341665680 -> 139944139995024
	139944139995024 [label=AccumulateGrad]
	139944139997088 -> 139944139993536
	139944341665600 [label="model.model.encoder.layer4.2.se_module.fc2.bias
 (2048)" fillcolor=lightblue]
	139944341665600 -> 139944139997088
	139944139997088 [label=AccumulateGrad]
	139944139994496 -> 139944139989072
	139944139990944 -> 139944139995456
	139944139990848 -> 139944139989168
	139944341570000 [label="model.model.decoder.blocks.0.conv1.0.weight
 (256, 3072, 3, 3)" fillcolor=lightblue]
	139944341570000 -> 139944139990848
	139944139990848 [label=AccumulateGrad]
	139944139995744 -> 139943686109216
	139943500611728 [label="model.model.decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	139943500611728 -> 139944139995744
	139944139995744 [label=AccumulateGrad]
	139944139994688 -> 139943686109216
	139944341757424 [label="model.model.decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	139944341757424 -> 139944139994688
	139944139994688 [label=AccumulateGrad]
	139943686110944 -> 139944475725824
	139944341762144 [label="model.model.decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139944341762144 -> 139943686110944
	139943686110944 [label=AccumulateGrad]
	139944475721984 -> 139944475727456
	139944341758064 [label="model.model.decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	139944341758064 -> 139944475721984
	139944475721984 [label=AccumulateGrad]
	139943686110320 -> 139944475727456
	139944341754464 [label="model.model.decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	139944341754464 -> 139943686110320
	139943686110320 [label=AccumulateGrad]
	139944475738016 -> 139944475737152
	139944475737584 -> 139944475735424
	139944341750304 [label="model.model.decoder.blocks.1.conv1.0.weight
 (128, 768, 3, 3)" fillcolor=lightblue]
	139944341750304 -> 139944475737584
	139944475737584 [label=AccumulateGrad]
	139944475735856 -> 139944475734560
	139944484362480 [label="model.model.decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	139944484362480 -> 139944475735856
	139944475735856 [label=AccumulateGrad]
	139944475733696 -> 139944475734560
	139944484361520 [label="model.model.decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	139944484361520 -> 139944475733696
	139944475733696 [label=AccumulateGrad]
	139944475732832 -> 139944475732400
	139944484358720 [label="model.model.decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139944484358720 -> 139944475732832
	139944475732832 [label=AccumulateGrad]
	139944475731104 -> 139944475731536
	139944484358160 [label="model.model.decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	139944484358160 -> 139944475731104
	139944475731104 [label=AccumulateGrad]
	139944475729808 -> 139944475731536
	139944484356480 [label="model.model.decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	139944484356480 -> 139944475729808
	139944475729808 [label=AccumulateGrad]
	139944475728944 -> 139944475728080
	139944475726784 -> 139944475726352
	139944484364000 [label="model.model.decoder.blocks.2.conv1.0.weight
 (64, 384, 3, 3)" fillcolor=lightblue]
	139944484364000 -> 139944475726784
	139944475726784 [label=AccumulateGrad]
	139944475725056 -> 139944475725488
	139944484362720 [label="model.model.decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	139944484362720 -> 139944475725056
	139944475725056 [label=AccumulateGrad]
	139944475724624 -> 139944475725488
	139944484359520 [label="model.model.decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	139944484359520 -> 139944475724624
	139944475724624 [label=AccumulateGrad]
	139944475723760 -> 139944475534016
	139944484371760 [label="model.model.decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139944484371760 -> 139944475723760
	139944475723760 [label=AccumulateGrad]
	139944475534112 -> 139944475532960
	139944484371680 [label="model.model.decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	139944484371680 -> 139944475534112
	139944475534112 [label=AccumulateGrad]
	139944475722032 -> 139944475532960
	139944484371600 [label="model.model.decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	139944484371600 -> 139944475722032
	139944475722032 [label=AccumulateGrad]
	139944475532576 -> 139944475538864
	139944475529936 -> 139944475528256
	139944484371120 [label="model.model.decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	139944484371120 -> 139944475529936
	139944475529936 [label=AccumulateGrad]
	139944475539008 -> 139944475539152
	139944484371040 [label="model.model.decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	139944484371040 -> 139944475539008
	139944475539008 [label=AccumulateGrad]
	139944475535168 -> 139944475539152
	139944484370960 [label="model.model.decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	139944484370960 -> 139944475535168
	139944475535168 [label=AccumulateGrad]
	139944475537040 -> 139944475528352
	139944484370320 [label="model.model.decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139944484370320 -> 139944475537040
	139944475537040 [label=AccumulateGrad]
	139944475537184 -> 139944475533008
	139944484370160 [label="model.model.decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	139944484370160 -> 139944475537184
	139944475537184 [label=AccumulateGrad]
	139944475534544 -> 139944475533008
	139944484370080 [label="model.model.decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	139944484370080 -> 139944475534544
	139944475534544 [label=AccumulateGrad]
	139944475527776 -> 139944475539392
	139944484369520 [label="model.model.decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	139944484369520 -> 139944475527776
	139944475527776 [label=AccumulateGrad]
	139944475539824 -> 139944475538528
	139944484369440 [label="model.model.decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	139944484369440 -> 139944475539824
	139944475539824 [label=AccumulateGrad]
	139944475537664 -> 139944475538528
	139944484369280 [label="model.model.decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	139944484369280 -> 139944475537664
	139944475537664 [label=AccumulateGrad]
	139944475536800 -> 139944475536368
	139944484368640 [label="model.model.decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139944484368640 -> 139944475536800
	139944475536800 [label=AccumulateGrad]
	139944475535072 -> 139944475532480
	139944484368560 [label="model.model.decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	139944484368560 -> 139944475535072
	139944475535072 [label=AccumulateGrad]
	139944475533776 -> 139944475532480
	139944484368400 [label="model.model.decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	139944484368400 -> 139944475533776
	139944475533776 [label=AccumulateGrad]
	139944475533344 -> 139944475531616
	139944484368000 [label="model.model.segmentation_head.0.weight
 (1, 16, 3, 3)" fillcolor=lightblue]
	139944484368000 -> 139944475533344
	139944475533344 [label=AccumulateGrad]
	139944475534640 -> 139944475531616
	139944484367920 [label="model.model.segmentation_head.0.bias
 (1)" fillcolor=lightblue]
	139944484367920 -> 139944475534640
	139944475534640 [label=AccumulateGrad]
	139944475531616 -> 139944476290848
}
